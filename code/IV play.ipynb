{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e33a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0a36801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from math import radians, cos, sin, asin, sqrt, atan2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import xy\n",
    "from rasterio.plot import show\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "from shapely.geometry import Point, Polygon, LineString, MultiLineString\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e7b109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/gleb/Desktop/thesis/data/'\n",
    "\n",
    "countries_path = data_path + 'world-administrative-boundaries/'\n",
    "basins_6_path = data_path + 'HydroBASINS Africa 6 level/'\n",
    "basins_12_path = data_path + 'HydroBASINS Africa 12 level/'\n",
    "conflict_path = data_path + 'conflicts/'\n",
    "dams_path = data_path + 'GRanD_Version_1_3/'\n",
    "sheds_file_3 = data_path + 'hyd_af_dem_3s/af_dem_3s.tif'\n",
    "sheds_file_15 = data_path + 'hyd_af_dem_15s/hyd_af_dem_15s.tif'\n",
    "rivers_file = data_path + 'HydroRIVERS_v10_af/HydroRIVERS_v10_af.gdb'\n",
    "\n",
    "output_path = '/home/gleb/Desktop/thesis/outcomes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6e9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers = gpd.read_file(rivers_file)\n",
    "levels = [1,2,3,4,5]\n",
    "rivers_main = rivers[rivers['ORD_CLAS'].apply(lambda x: True if x in levels else False)].copy()\n",
    "rivers_main.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0789ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(sheds_file_15) as dem:\n",
    "    dem_data = dem.read(1)\n",
    "    transformation = dem.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ebe701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_riv_gradient(elevation_data, transform, line_string):\n",
    "    points = [xy for xy in line_string.coords]\n",
    "    gradient_values = []\n",
    "    distance_values = []\n",
    "    for idx in range(len(points) - 1):\n",
    "        point1, point2 = points[idx], points[idx + 1]\n",
    "        col1, row1 = rasterio.transform.rowcol(transformation, point1[0], point1[1])\n",
    "        col2, row2 = rasterio.transform.rowcol(transformation, point2[0], point2[1])\n",
    "        elevation1 = dem_data[col1, row1]\n",
    "#         print('Elevation level of point 1: ' + str(elevation1))\n",
    "        elevation2 = dem_data[col2, row2]\n",
    "#         print('Elevation level of point 2: ' + str(elevation2))\n",
    "        distance = haversine(point1[0], point1[1], point2[0], point2[1])\n",
    "#         print('Distance between points: ' + str(distance))\n",
    "        gradient = ((elevation1 - elevation2) / distance)*100\n",
    "#         print('Gradient between points: ' + str(gradient))\n",
    "        distance_values.append(distance)\n",
    "        gradient_values.append(gradient)\n",
    "    return gradient_values, distance_values\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 * 1000\n",
    "    return c * r\n",
    "\n",
    "def gradient_fits(grad, category='suitable'):\n",
    "    if category=='suitable':\n",
    "        if (grad >= 1.5) & (grad <= 3):\n",
    "            return True\n",
    "        elif grad >= 6:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif category=='1.5-3':\n",
    "        if (grad >= 1.5) & (grad <= 3):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif category == '3-6':\n",
    "        if (grad > 3) & (grad < 6):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif category == '>6':\n",
    "        if grad >= 6:\n",
    "            return True \n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2913da3",
   "metadata": {},
   "source": [
    "# Test for river distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e6c2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7221.239512363846"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if isinstance(river_geoms, MultiLineString):\n",
    "#     grads, dists = calculate_gradient(dem_data, transformation, river_geoms[0])\n",
    "index = 24740\n",
    "river_geoms = rivers_main.loc[index, 'geometry']\n",
    "grads, dists = calculate_riv_gradient(dem_data, transformation, river_geoms.geoms[0])\n",
    "sum(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5228315b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HYRIV_ID</th>\n",
       "      <th>NEXT_DOWN</th>\n",
       "      <th>MAIN_RIV</th>\n",
       "      <th>LENGTH_KM</th>\n",
       "      <th>DIST_DN_KM</th>\n",
       "      <th>DIST_UP_KM</th>\n",
       "      <th>CATCH_SKM</th>\n",
       "      <th>UPLAND_SKM</th>\n",
       "      <th>ENDORHEIC</th>\n",
       "      <th>DIS_AV_CMS</th>\n",
       "      <th>ORD_STRA</th>\n",
       "      <th>ORD_CLAS</th>\n",
       "      <th>ORD_FLOW</th>\n",
       "      <th>HYBAS_L12</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24740</th>\n",
       "      <td>10025303</td>\n",
       "      <td>10024963</td>\n",
       "      <td>10018841</td>\n",
       "      <td>7.22</td>\n",
       "      <td>137.100006</td>\n",
       "      <td>11.1</td>\n",
       "      <td>25.48</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.120109e+09</td>\n",
       "      <td>0.07092</td>\n",
       "      <td>MULTILINESTRING ((-6.55833 33.26250, -6.55208 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       HYRIV_ID  NEXT_DOWN  MAIN_RIV  LENGTH_KM  DIST_DN_KM  DIST_UP_KM  \\\n",
       "24740  10025303   10024963  10018841       7.22  137.100006        11.1   \n",
       "\n",
       "       CATCH_SKM  UPLAND_SKM  ENDORHEIC  DIS_AV_CMS  ORD_STRA  ORD_CLAS  \\\n",
       "24740      25.48        25.5          0       0.047         1         2   \n",
       "\n",
       "       ORD_FLOW     HYBAS_L12  Shape_Length  \\\n",
       "24740         8  1.120109e+09       0.07092   \n",
       "\n",
       "                                                geometry  \n",
       "24740  MULTILINESTRING ((-6.55833 33.26250, -6.55208 ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rivers_main.iloc[index:index+1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9b60a",
   "metadata": {},
   "source": [
    "# Basins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c4a9e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bas_6 = gpd.read_file(basins_6_path + 'hybas_af_lev06_v1c.shp')\n",
    "bas_12 = gpd.read_file(basins_12_path + 'hybas_af_lev12_v1c.shp')\n",
    "bas_12['PFAF_ID_6l'] = bas_12['PFAF_ID'].apply(lambda x: int(str(x)[:6]))\n",
    "bas_l12_to_6 = pd.Series(bas_12.PFAF_ID_6l.values,index=bas_12.HYBAS_ID).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IV and geographic controls for each basin: \n",
    "# 1. Revir Gradient = ratio_of_grad, check\n",
    "# 2. basin size = SUB_AREA (in sq.km), check\n",
    "# 3. elevation = average_elevation, check\n",
    "# 4. average basin gradient = average_gradient, check\n",
    "# 5. river length= tot_riv_dist, check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b87b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching rivers to basins of 6th level using PFAF ids:\n",
    "rivers['HYBAS_L6_PFAFID'] = rivers['HYBAS_L12'].apply(lambda x: bas_l12_to_6[x])\n",
    "rivers_main['HYBAS_L6_PFAFID'] = rivers_main['HYBAS_L12'].apply(lambda x: bas_l12_to_6[x])\n",
    "\n",
    "# Dictionary of rivers geometries. Keys: rivers' HYRIV_ID. Values: geometries.\n",
    "riv_geoms = pd.Series(rivers.geometry.values,index=rivers.HYRIV_ID).to_dict()\n",
    "\n",
    "# Dictionary basins-rivers. Keys: Basin's PFAF index. Values: lists of 'HYRIV_ID' that belong to a basin.\n",
    "bas_riv_dict = rivers_main.groupby('HYBAS_L6_PFAFID')['HYRIV_ID'].apply(list).to_dict()\n",
    "# bas_riv_dict = rivers.groupby('HYBAS_L6_PFAFID')['HYRIV_ID'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35c9563b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated ratio of IDs: 0.99972199054767864543\r"
     ]
    }
   ],
   "source": [
    "# Calculate ratios of suitable river gradients and total length of rivers for basins:\n",
    "ratios_RG = {}\n",
    "ratios_15_3 = {}\n",
    "ratios_3_6 = {}\n",
    "ratios_more_6 = {}\n",
    "basin_riv_dist = {}\n",
    "\n",
    "for pfaf_id in set(bas_6.PFAF_ID):\n",
    "    print('Calculated ratio of IDs: ' + str(len(ratios_RG)/len(set(bas_6.PFAF_ID))), end = '\\r')\n",
    "    if pfaf_id in bas_riv_dict.keys():\n",
    "        rivers_in_basin = bas_riv_dict[pfaf_id]\n",
    "        \n",
    "        total_rivers_distance = 0\n",
    "        fitted_gradients_distance_RG = 0\n",
    "        fitted_gradients_distance_15_3 = 0\n",
    "        fitted_gradients_distance_3_6 = 0\n",
    "        fitted_gradients_distance_more_6 = 0\n",
    "        \n",
    "        for riv in rivers_in_basin:\n",
    "            river_geom = riv_geoms[riv]\n",
    "            grads, dists = calculate_riv_gradient(dem_data, transformation, river_geom.geoms[0])\n",
    "            # Which sections of the river have gradient 1.5-3% or >3%:\n",
    "            grad_fits_RG = [gradient_fits(grad, category='suitable') for grad in grads]\n",
    "            grad_fits_15_3 = [gradient_fits(grad, category='1.5-3') for grad in grads]\n",
    "            grad_fits_3_6 = [gradient_fits(grad, category='3-6') for grad in grads]\n",
    "            grad_fits_more_6 = [gradient_fits(grad, category='>6') for grad in grads]\n",
    "            # Calculate distance of sections within needed gradient:\n",
    "            fitted_gradients_distance_RG += sum([x for x, y in zip(dists, grad_fits_RG) if y])\n",
    "            fitted_gradients_distance_15_3 += sum([x for x, y in zip(dists, grad_fits_15_3) if y])\n",
    "            fitted_gradients_distance_3_6 += sum([x for x, y in zip(dists, grad_fits_3_6) if y])\n",
    "            fitted_gradients_distance_more_6 += sum([x for x, y in zip(dists, grad_fits_more_6) if y])\n",
    "            # Add length of the river to the distance of all rivers in the basin:\n",
    "            total_rivers_distance += sum(dists)\n",
    "        ratio_RG = fitted_gradients_distance_RG/total_rivers_distance\n",
    "        ratio_15_3 = fitted_gradients_distance_15_3/total_rivers_distance\n",
    "        ratio_3_6 = fitted_gradients_distance_3_6/total_rivers_distance\n",
    "        ratio_more_6 = fitted_gradients_distance_more_6/total_rivers_distance\n",
    "        \n",
    "        ratios_RG[pfaf_id] = ratio_RG\n",
    "        ratios_15_3[pfaf_id] = ratio_15_3\n",
    "        ratios_3_6[pfaf_id] = ratio_3_6\n",
    "        ratios_more_6[pfaf_id] = ratio_more_6\n",
    "        basin_riv_dist[pfaf_id] = total_rivers_distance\n",
    "    else:\n",
    "        ratios_RG[pfaf_id] = 0\n",
    "        ratios_15_3[pfaf_id] = 0\n",
    "        ratios_3_6[pfaf_id] = 0\n",
    "        ratios_more_6[pfaf_id] = 0\n",
    "        basin_riv_dist[pfaf_id] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af21197",
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_6['RG'] = bas_6['PFAF_ID'].apply(lambda x: ratios_RG[x])\n",
    "bas_6['grad_15_3'] = bas_6['PFAF_ID'].apply(lambda x: ratios_15_3[x])\n",
    "bas_6['grad_3_6'] = bas_6['PFAF_ID'].apply(lambda x: ratios_3_6[x])\n",
    "bas_6['grad_more_6'] = bas_6['PFAF_ID'].apply(lambda x: ratios_more_6[x])\n",
    "bas_6['tot_riv_dist'] = bas_6['PFAF_ID'].apply(lambda x: int(basin_riv_dist[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67057b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bas_6.to_file(output_path + 'hybas_af_lev06_v1c_grads.shp')\n",
    "bas_6 = gpd.read_file(output_path + 'hybas_af_lev06_v1c_grads.shp')\n",
    "bas_6.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "31388e3f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Calculate average elevation and gradient for basins:\n",
    "# def calculate_bas_controls(geom, dem_file=dem_data):\n",
    "#     mask = geometry_mask([geom], out_shape=dem_file.shape, transform=transformation,\n",
    "#                          all_touched=False, invert=False)\n",
    "#     # Calculate average elevation:\n",
    "#     basin_elevations = np.ma.array(dem_data, mask=mask)\n",
    "#     av_elevation = basin_elevations.mean()\n",
    "# #     print('Elevation calculated') \n",
    "#     # Calculate average gradient: \n",
    "#     max_indices = np.unravel_index(np.argmax(basin_elevations), basin_elevations.shape)\n",
    "#     min_indices = np.unravel_index(np.argmin(basin_elevations), basin_elevations.shape)\n",
    "#     max_lon, max_lat = xy(transformation, max_indices[0], max_indices[1])\n",
    "#     min_lon, min_lat = xy(transformation, min_indices[0], min_indices[1])\n",
    "#     distance = haversine(min_lon, min_lat, max_lon, max_lat)\n",
    "#     av_gradient = ((basin_elevations[max_indices] - basin_elevations[min_indices]) / distance)*100\n",
    "# #     print('Gradient calculated')\n",
    "#     return [av_elevation, av_gradient]\n",
    "\n",
    "# bas_6[['av_elev','av_grad']] = pd.DataFrame(bas_6['geometry'].apply(lambda x: \n",
    "#                                                                     calculate_bas_controls(x)).tolist(),\n",
    "#                                                                index= bas_6.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef05b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average elevation and gradient for basins:\n",
    "def calculate_bas_controls(geom, dem_file=dem_data):\n",
    "    mask = geometry_mask([geom], out_shape=dem_file.shape, transform=transformation,\n",
    "                         all_touched=False, invert=False)\n",
    "    # Calculate average elevation:\n",
    "    basin_elevations = np.ma.array(dem_data, mask=mask)\n",
    "    av_elevation = basin_elevations.mean()\n",
    "    basin_elevations = basin_elevations[~basin_elevations.mask].data\n",
    "    n_pixels = len(basin_elevations)\n",
    "    pxs_25 = len(basin_elevations[basin_elevations<=250])\n",
    "    pxs_25_50 = len(basin_elevations[(basin_elevations>250) & (basin_elevations<=500)])\n",
    "    pxs_50_1k = len(basin_elevations[(basin_elevations>500) & (basin_elevations<=1000)])\n",
    "    pxs_more_1k = len(basin_elevations[(basin_elevations>1000) & (basin_elevations<=10000)])\n",
    "    share_less_25 = pxs_25/n_pixels\n",
    "    share_25_50 = pxs_25_50/n_pixels\n",
    "    share_50_1k = pxs_50_1k/n_pixels\n",
    "    share_more_1k = pxs_more_1k/n_pixels\n",
    "    return [av_elevation, share_less_25, share_25_50, share_50_1k, share_more_1k]\n",
    "\n",
    "bas_6[['av_elev', 'shr_l_25', 'shr_25_50', 'shr_50_1k', 'shr_m_1k']] = pd.DataFrame(bas_6['geometry'].apply(\n",
    "    lambda x: calculate_bas_controls(x)).tolist(), index= bas_6.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1419e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6.to_file(output_path + '5rivers_controls.shp')\n",
    "bas_6 = gpd.read_file(output_path + '5rivers_controls.shp')\n",
    "bas_6.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist'}, inplace=True)\n",
    "bas_6['HYBAS_ID'] = bas_6['HYBAS_ID'].apply(lambda x: str(x))\n",
    "bas_6['PFAF_ID'] = bas_6['PFAF_ID'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711b112",
   "metadata": {},
   "source": [
    "# Dams for 1st stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c40540a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dams_df = gpd.read_file(dams_path + 'GRanD_dams_v1_3.shp')\n",
    "# Wherever main year is -99, so is alternative year:\n",
    "dams_df = dams_df[dams_df['YEAR'] != -99]\n",
    "dams_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # dams_df[(dams_df['YEAR'] == -99)&(dams_df['COUNTRY'] == 'Zimbabwe')]\n",
    "# a = dams_df[(dams_df['YEAR'] == -99)][['RES_NAME', 'DAM_NAME', 'ALT_NAME', 'RIVER',\n",
    "#                                        'ALT_RIVER', 'GRAND_ID', 'COUNTRY', 'USE_ELEC', 'USE_IRRI']].copy()\n",
    "\n",
    "# uses = {'Main', 'Sec', 'Major'}\n",
    "# a['el_ir'] = a.apply(lambda row: 1 if (row.USE_ELEC in uses) or \n",
    "#                                               (row.USE_IRRI in uses) else 0, axis=1)\n",
    "\n",
    "# print(a[a['el_ir']==1]['COUNTRY'].value_counts())\n",
    "\n",
    "# a = a[a['el_ir']==1]\n",
    "# a.reset_index(drop=True,inplace=True)\n",
    "\n",
    "# Nigeria: 8,\n",
    "# Zimbabwe: 7,\n",
    "# South Africa: 3,\n",
    "# Burkina Faso: 2,\n",
    "# Tunisia: 2,\n",
    "# Egypt: 2,\n",
    "# Ethiopia: 1,\n",
    "# Lesotho: 1,\n",
    "# Botswana: 2,\n",
    "# Mozambique: 1,\n",
    "# Zambia: 1,\n",
    "# Namibia: 1,\n",
    "# Togo: 1,\n",
    "# Ivory Coast: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1c9ca9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_dams_97 = dams_df[dams_df['YEAR'] <= 1999].copy()\n",
    "any_dams_97.reset_index(inplace=True,drop=True)\n",
    "any_dams_89 = dams_df[dams_df['YEAR'] <= 1989].copy()\n",
    "any_dams_89.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# uses = {'Major'}\n",
    "uses = {'Main', 'Major'}\n",
    "# uses = {'Main', 'Sec', 'Major'}\n",
    "\n",
    "any_dams_97['el_ir'] = any_dams_97.apply(lambda row: 1 if (row.USE_ELEC in uses) or \n",
    "                                              (row.USE_IRRI in uses) else 0, axis=1)\n",
    "any_dams_97['el'] = any_dams_97.apply(lambda row: 1 if row.USE_ELEC in uses else 0, axis=1)\n",
    "any_dams_97['ir'] = any_dams_97.apply(lambda row: 1 if row.USE_IRRI in uses else 0, axis=1)\n",
    "\n",
    "any_dams_89['el_ir'] = any_dams_89.apply(lambda row: 1 if (row.USE_ELEC in uses) or \n",
    "                                              (row.USE_IRRI in uses) else 0, axis=1)\n",
    "any_dams_89['el'] = any_dams_89.apply(lambda row: 1 if row.USE_ELEC in uses else 0, axis=1)\n",
    "any_dams_89['ir'] = any_dams_89.apply(lambda row: 1 if row.USE_IRRI in uses else 0, axis=1)\n",
    "\n",
    "dams_el_ir_97 = any_dams_97[any_dams_97['el_ir'] == 1]\n",
    "dams_el_ir_97.reset_index(inplace=True,drop=True)\n",
    "dams_el_97 = any_dams_97[any_dams_97['el'] == 1]\n",
    "dams_el_97.reset_index(inplace=True,drop=True)\n",
    "dams_ir_97 = any_dams_97[any_dams_97['ir'] == 1]\n",
    "dams_ir_97.reset_index(inplace=True,drop=True)\n",
    "\n",
    "dams_el_ir_89 = any_dams_89[any_dams_89['el_ir'] == 1]\n",
    "dams_el_ir_89.reset_index(inplace=True,drop=True)\n",
    "dams_el_89 = any_dams_89[any_dams_89['el'] == 1]\n",
    "dams_el_89.reset_index(inplace=True,drop=True)\n",
    "dams_ir_89 = any_dams_89[any_dams_89['ir'] == 1]\n",
    "dams_ir_89.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c3960d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  0.102507  0.047256\n",
      "1     grad_3_6  0.015465  0.885657\n",
      "2  grad_more_6  0.153903  0.069869\n",
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  0.082220  0.222317\n",
      "1     grad_3_6  0.128011  0.361449\n",
      "2  grad_more_6  0.163503  0.139737\n"
     ]
    }
   ],
   "source": [
    "# Hydropower dams before 1997:\n",
    "joined = gpd.sjoin(dams_el_97[['GRAND_ID', 'YEAR', 'MAIN_USE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids\n",
    "    \n",
    "bas_6['has_dam'] = bas_6['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "bas_6['n_of_dams'] = bas_6['HYBAS_ID'].apply(lambda x: len(basin_dam_dict[x]) if x in \n",
    "                                                  basin_dam_dict.keys() else 0)\n",
    "\n",
    "first_stage1 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "first_stage2 = smf.ols(\"n_of_dams ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "\n",
    "rows, model = [], first_stage1\n",
    "for idx in range (len(first_stage1.params)-3, len(first_stage1.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))\n",
    "\n",
    "rows, model = [], first_stage2\n",
    "for idx in range (len(first_stage2.params)-3, len(first_stage2.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5966d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  0.116317  0.021539\n",
      "1     grad_3_6 -0.033116  0.753227\n",
      "2  grad_more_6  0.153571  0.064769\n",
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  0.096030  0.149253\n",
      "1     grad_3_6  0.079430  0.566657\n",
      "2  grad_more_6  0.163171  0.135912\n"
     ]
    }
   ],
   "source": [
    "# Hydropower dams before 1989:\n",
    "joined = gpd.sjoin(dams_el_89[['GRAND_ID', 'YEAR', 'MAIN_USE', 'TIMELINE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids\n",
    "    \n",
    "bas_6['has_dam'] = bas_6['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "bas_6['n_of_dams'] = bas_6['HYBAS_ID'].apply(lambda x: len(basin_dam_dict[x]) if x in \n",
    "                                                  basin_dam_dict.keys() else 0)\n",
    "\n",
    "first_stage3 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "\n",
    "first_stage4 = smf.ols(\"n_of_dams ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "\n",
    "rows, model = [], first_stage3\n",
    "for idx in range (len(first_stage3.params)-3, len(first_stage3.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))\n",
    "\n",
    "rows, model = [], first_stage4\n",
    "for idx in range (len(first_stage2.params)-3, len(first_stage2.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68717f49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  0.350065  0.000604\n",
      "1     grad_3_6  0.429852  0.042999\n",
      "2  grad_more_6 -0.373268  0.025983\n",
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  1.037230  0.000180\n",
      "1     grad_3_6  0.513601  0.372638\n",
      "2  grad_more_6 -0.580859  0.201433\n"
     ]
    }
   ],
   "source": [
    "# Irrigation dams before 1997:\n",
    "joined = gpd.sjoin(dams_ir_97[['GRAND_ID', 'YEAR', 'MAIN_USE', 'TIMELINE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids\n",
    "    \n",
    "bas_6['has_dam'] = bas_6['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "bas_6['n_of_dams'] = bas_6['HYBAS_ID'].apply(lambda x: len(basin_dam_dict[x]) if x in \n",
    "                                                  basin_dam_dict.keys() else 0)\n",
    "\n",
    "# Only dams for irrigation or electricity:\n",
    "\n",
    "first_stage5 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "first_stage6 = smf.ols(\"n_of_dams ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "\n",
    "rows, model = [], first_stage5\n",
    "for idx in range (len(first_stage5.params)-3, len(first_stage5.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))\n",
    "\n",
    "rows, model = [], first_stage6\n",
    "for idx in range (len(first_stage6.params)-3, len(first_stage6.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11ccd86b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  0.360826  0.000357\n",
      "1     grad_3_6  0.389104  0.064302\n",
      "2  grad_more_6 -0.362933  0.028797\n",
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  1.067190  0.000082\n",
      "1     grad_3_6  0.347510  0.537473\n",
      "2  grad_more_6 -0.576698  0.194812\n"
     ]
    }
   ],
   "source": [
    "# Irrigation dams before 1989:\n",
    "joined = gpd.sjoin(dams_ir_89[['GRAND_ID', 'YEAR', 'MAIN_USE', 'TIMELINE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids\n",
    "    \n",
    "bas_6['has_dam'] = bas_6['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "bas_6['n_of_dams'] = bas_6['HYBAS_ID'].apply(lambda x: len(basin_dam_dict[x]) if x in \n",
    "                                                  basin_dam_dict.keys() else 0)\n",
    "\n",
    "# Only dams for irrigation or electricity:\n",
    "\n",
    "first_stage7 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "first_stage8 = smf.ols(\"n_of_dams ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "\n",
    "rows, model = [], first_stage7\n",
    "for idx in range (len(first_stage7.params)-3, len(first_stage5.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))\n",
    "\n",
    "rows, model = [], first_stage8\n",
    "for idx in range (len(first_stage5.params)-3, len(first_stage5.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b34ea808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  0.412424  0.000182\n",
      "1     grad_3_6  0.481213  0.035862\n",
      "2  grad_more_6 -0.220129  0.223777\n",
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  1.112894  0.000096\n",
      "1     grad_3_6  0.650249  0.273350\n",
      "2  grad_more_6 -0.425002  0.364328\n"
     ]
    }
   ],
   "source": [
    "# Hyrdopower and irrigation dams before 1997:\n",
    "joined = gpd.sjoin(dams_el_ir_97[['GRAND_ID', 'YEAR', 'MAIN_USE', 'TIMELINE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids\n",
    "    \n",
    "bas_6['has_dam'] = bas_6['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "bas_6['n_of_dams'] = bas_6['HYBAS_ID'].apply(lambda x: len(basin_dam_dict[x]) if x in \n",
    "                                                  basin_dam_dict.keys() else 0)\n",
    "\n",
    "# Only dams for irrigation or electricity:\n",
    "\n",
    "first_stage9 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "first_stage10 = smf.ols(\"n_of_dams ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "\n",
    "rows, model = [], first_stage9\n",
    "for idx in range (len(first_stage9.params)-3, len(first_stage9.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))\n",
    "\n",
    "rows, model = [], first_stage10\n",
    "for idx in range (len(first_stage10.params)-3, len(first_stage10.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0fb4182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  0.497319  0.000042\n",
      "1     grad_3_6  0.449326  0.075291\n",
      "2  grad_more_6 -0.346480  0.082237\n",
      "                   Coef:     P>|t|\n",
      "0    grad_15_3  1.861760  0.000031\n",
      "1     grad_3_6  0.224746  0.808960\n",
      "2  grad_more_6 -1.177234  0.108648\n"
     ]
    }
   ],
   "source": [
    "# Hyrdopower and irrigation dams before 1989:\n",
    "joined = gpd.sjoin(any_dams_89[['GRAND_ID', 'YEAR', 'MAIN_USE', 'TIMELINE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids\n",
    "\n",
    "bas_6['has_dam'] = bas_6['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "bas_6['n_of_dams'] = bas_6['HYBAS_ID'].apply(lambda x: len(basin_dam_dict[x]) if x in \n",
    "                                                  basin_dam_dict.keys() else 0)\n",
    "\n",
    "# Only dams for irrigation or electricity:\n",
    "\n",
    "first_stage11 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "first_stage12 = smf.ols(\"n_of_dams ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_more_6\",\n",
    "                      data=bas_6).fit()\n",
    "\n",
    "rows, model = [], first_stage11\n",
    "for idx in range (len(first_stage11.params)-3, len(first_stage11.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))\n",
    "\n",
    "rows, model = [], first_stage12\n",
    "for idx in range (len(first_stage12.params)-3, len(first_stage12.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], float(model.pvalues[idx])])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Coef:         P>|t|\n",
      "0      shr_m_1k -4.179289e-03  8.851845e-01\n",
      "1  tot_riv_dist  9.009867e-09  6.398244e-03\n",
      "2            RG  1.579615e-01  7.058144e-07\n"
     ]
    }
   ],
   "source": [
    "# Hydropower dams before 1997:\n",
    "joined = gpd.sjoin(dams_el_97[['GRAND_ID', 'YEAR', 'MAIN_USE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids\n",
    "    \n",
    "bas_6['has_dam'] = bas_6['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "bas_6['n_of_dams'] = bas_6['HYBAS_ID'].apply(lambda x: len(basin_dam_dict[x]) if x in \n",
    "                                                  basin_dam_dict.keys() else 0)\n",
    "\n",
    "first_stage1 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + RG\",\n",
    "                      data=bas_6).fit()\n",
    "first_stage2 = smf.ols(\"n_of_dams ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + RG\",\n",
    "                      data=bas_6).fit()\n",
    "\n",
    "# print(\"ratio_of_grad parameter estimate:, \", first_stage.params[\"RG\"])\n",
    "# print(\"ratio_of_grad p-value:, \", first_stage.pvalues[\"RG\"])\n",
    "# print(first_stage1.summary())\n",
    "# print(first_stage2.summary())\n",
    "\n",
    "# rows, model = [], first_stage1\n",
    "# for idx in range (len(first_stage1.params)-3, len(first_stage1.params)):\n",
    "#     rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "# print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))\n",
    "\n",
    "rows, model = [], first_stage2\n",
    "for idx in range (len(first_stage2.params)-3, len(first_stage2.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea5e2d",
   "metadata": {},
   "source": [
    "## Assign basins to the countries and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d63c915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_temp = bas_6[['HYBAS_ID', 'geometry']].copy()\n",
    "# Create centroids of basins:\n",
    "bas_temp[\"geometry\"] = bas_temp[\"geometry\"].centroid\n",
    "# Read boundaries of countries:\n",
    "countries = gpd.read_file(countries_path + 'world-administrative-boundaries.shp')\n",
    "cntry_name_match = {'Bosnia & Herzegovina':'Bosnia and Herzegovina',\n",
    "                    'Democratic Republic of the Congo':'Congo (DRC)',\n",
    "                    'Iran (Islamic Republic of)':'Iran',\n",
    "                    \"CÃ´te d'Ivoire\":'Ivory Coast',\"Lao People's Democratic Republic\":'Laos',\n",
    "                    'Libyan Arab Jamahiriya':'Libya', \"Ma'tan al-Sarra\":'Libya',\n",
    "                    'The former Yugoslav Republic of Macedonia':'Macedonia',\n",
    "                    'Moldova, Republic of':'Moldova', 'Myanmar':'Myanmar (Burma)',\n",
    "                    \"Democratic People's Republic of Korea\": 'North Korea',\n",
    "                    'Russian Federation':'Russia', 'Republic of Korea':'South Korea',\n",
    "                    'Syrian Arab Republic':'Syria', 'United Republic of Tanzania':'Tanzania',\n",
    "                    'U.K. of Great Britain and Northern Ireland':'United Kingdom','United States of America':'United States'}\n",
    "\n",
    "countries['name'] = countries['name'].apply(lambda x: cntry_name_match[x] if x in cntry_name_match.keys() else x)\n",
    "afr_countries = countries[countries.continent == 'Africa'].copy()\n",
    "afr_countries.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Match centroids of basins with countries:\n",
    "joined = gpd.sjoin(bas_temp, afr_countries, how='left', op='within')\n",
    "joined = joined[~joined[['index_right']].isna().any(axis=1)].reset_index(drop=True)\n",
    "bas_country_dict = pd.Series(joined.name.values,index=joined.HYBAS_ID).to_dict()\n",
    "\n",
    "# Create lists of basins for each corresponding country:\n",
    "country_bas_dict = {}\n",
    "for country, basins_in_country in joined.groupby('name'):\n",
    "    bas_ids = basins_in_country['HYBAS_ID'].to_list()\n",
    "    country_bas_dict[country] = bas_ids\n",
    "    \n",
    "# Hold accounts of not assigned basins\n",
    "not_assigned_bas = set(joined[joined[['iso3']].isna().any(axis=1)]['HYBAS_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually add coastal basins for which centroid lies outside of the country borders:\n",
    "# Extend not assigned basins dict:\n",
    "not_assigned_bas = {'1060008110': 'Somalia',\n",
    "                    # '1060003780':,\n",
    "                    '1060020500': 'Gabon',\n",
    "                    '1060023020': 'Nigeria',\n",
    "                    '1060032860': 'Libyan Arab Jamahiriya',\n",
    "                    # '1060034490': 'Sao Tome and Principe',\n",
    "                    # '1060034610': ,\n",
    "                    # '1060034900': ,\n",
    "                    '1060035090': 'United Republic of Tanzania',\n",
    "                    # '1060040030': 'Madagascar',\n",
    "                    # '1060040040': 'Madagascar',\n",
    "                    # '1060040050': 'Comoros',\n",
    "                    # '1060040110': 'Seychelles',\n",
    "                    # '1060040140': 'Seychelles',\n",
    "                    # '1060040160': ,\n",
    "                    # '1060040180':\n",
    "                    }\n",
    "\n",
    "# Apply dicts to fill the countries for basins:\n",
    "bas_6['Country'] = bas_6['HYBAS_ID'].apply(lambda x: bas_country_dict.get(x, 'None'))\n",
    "bas_6['Country'] = bas_6['Country'].apply(lambda x: not_assigned_bas.get(x, x))\n",
    "\n",
    "# Drop basins w/o assigned countries (these basins have coastal and island nature): \n",
    "bas_6 = bas_6[bas_6['Country'] != 'None']\n",
    "bas_6.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the disputed territories:\n",
    "country_adjust = {\"Hala'ib Triangle\": 'Egypt',\n",
    "                  'Ilemi Triangle': 'Kenya'}\n",
    "bas_6['Country'] = bas_6['Country'].apply(lambda x: country_adjust[x] if x in country_adjust.keys() else x)\n",
    "\n",
    "# # Following the identification strategy, I remove countries which occupy only 1 hydrobasin\n",
    "# Find countries with 1 hydrobasin:\n",
    "countries_to_remove = set()\n",
    "for country in country_bas_dict.keys():\n",
    "    if len(country_bas_dict[country]) == 1:\n",
    "        countries_to_remove.add(country)\n",
    "\n",
    "# {'Burundi', 'Djibouti', 'Ilemi Triangle'}\n",
    "\n",
    "bas_6 = bas_6[~bas_6['Country'].isin(countries_to_remove)].copy()\n",
    "bas_6.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6.to_file(output_path + '5rivers_all_controls.shp')\n",
    "bas_6 = gpd.read_file(output_path + '5rivers_all_controls.shp')\n",
    "bas_6.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist'}, inplace=True)\n",
    "bas_6['HYBAS_ID'] = bas_6['HYBAS_ID'].apply(lambda x: str(x))\n",
    "bas_6['PFAF_ID'] = bas_6['PFAF_ID'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the basin data frame by month (1-12) and year (1997-2023) to a panel dataset:\n",
    "bas_6['year'] = bas_6['HYBAS_ID'].apply(lambda x: [year for year in range(1999, 2024)])\n",
    "bas_6_exp = bas_6.explode('year', ignore_index=True)\n",
    "# bas_6_exp['month'] = bas_6_exp['HYBAS_ID'].apply(lambda x: [month for month in range(1, 13)])\n",
    "# bas_6_exp = bas_6_exp.explode('month', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match dams per country per year with basins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1987: [1918],\n",
       " 2002: [2085],\n",
       " 2003: [769],\n",
       " 2005: [2006],\n",
       " 2007: [820, 2844],\n",
       " 2008: [772, 2882],\n",
       " 2013: [6315],\n",
       " 2016: [6285]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IDs of dams that were removed in corresponding years:\n",
    "rem_dams = dams_df[dams_df['REM_YEAR'] != -99].copy().reset_index(drop=True)\n",
    "# rem_dams = pd.Series(rem_dams.REM_YEAR.values,index=rem_dams.GRAND_ID).to_dict()\n",
    "year_rem_dict = {}\n",
    "for year, data in rem_dams.groupby('REM_YEAR'):\n",
    "    dam_ids = data['GRAND_ID'].to_list()\n",
    "    year_rem_dict[year] = dam_ids\n",
    "# year_rem_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f3d2bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate total number of dams in country c in year t:\n",
    "# Split the dams df:\n",
    "year_to_use = 1999\n",
    "dams_before_start = dams_df[dams_df['YEAR'] < year_to_use]\n",
    "\n",
    "# Create a dict of countries and dams:\n",
    "dams_p_cntr_at_start = dams_before_start.groupby('COUNTRY').count().reset_index()\n",
    "dams_p_cntr_at_start = pd.Series(dams_p_cntr_at_start.GRAND_ID.values,\n",
    "                                index=dams_p_cntr_at_start.COUNTRY).to_dict()\n",
    "\n",
    "# Create column in for dams per country in 1999:\n",
    "bas_6_exp['dams_per_count_991'] = bas_6_exp['Country'].apply(lambda x: dams_p_cntr_at_start.get(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99998882369376924646\r"
     ]
    }
   ],
   "source": [
    "# # Calculate dams for country c for year t:\n",
    "# Split the df:\n",
    "dams_after_start = dams_df[dams_df['YEAR'] >= year_to_use]\n",
    "dams_after_start = dams_after_start.groupby(['COUNTRY', 'YEAR']).count().reset_index()\n",
    "\n",
    "# Create the dict where key is tuple of country and year:\n",
    "new_dams_after_start = dict()\n",
    "for index, row in dams_after_start.iterrows():\n",
    "    key = (row['COUNTRY'], row['YEAR'])\n",
    "    new_dams_after_start[key] = row['GRAND_ID']\n",
    "\n",
    "# Create the dict of total amount of dams in country c for year t:\n",
    "counts = set(bas_6_exp.Country)\n",
    "tot_dams_per_country_year = {}\n",
    "idx = 0\n",
    "for country in counts:\n",
    "    idx += 1\n",
    "    print(idx/len(counts), end='\\r')\n",
    "    tot_dams_per_year = {}\n",
    "    for year in range(1999, 2024):\n",
    "        if year == 1999:\n",
    "            if country in dams_p_cntr_at_start.keys():\n",
    "                tot_dams_per_year[year] = dams_p_cntr_at_start[country]\n",
    "            else:\n",
    "                tot_dams_per_year[year] = 0\n",
    "        else:\n",
    "            key = (country, year)\n",
    "            if key in new_dams_after_start.keys():\n",
    "                tot_dams_per_year[year] = tot_dams_per_year[year-1] + new_dams_after_start[key]\n",
    "            else:\n",
    "                tot_dams_per_year[year] = tot_dams_per_year[year-1]\n",
    "    tot_dams_per_country_year[country] = tot_dams_per_year\n",
    "    \n",
    "# Use the created dictionary to fill in the basins dataframe:\n",
    "bas_6_exp['n_of_dam_in_c_per_y'] = 0\n",
    "for idx, row in bas_6_exp.iterrows():\n",
    "    print(idx/len(bas_6_exp), end='\\r')\n",
    "    bas_6_exp.loc[idx, 'n_of_dam_in_c_per_y'] = tot_dams_per_country_year[row['Country']][row['year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Left to-dos:\n",
    "# 1. Account for removed dams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6_exp.to_file(output_path + '5rivers_all_controls_panel.shp')\n",
    "bas_6_exp = gpd.read_file(output_path + '5rivers_all_controls_panel.shp')\n",
    "bas_6_exp.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist',\n",
    "                      'n_of_dam_i':'dams_in_c_per_y', 'dams_per_c':'dams_per_c_99'}, inplace=True)\n",
    "bas_6_exp['HYBAS_ID'] = bas_6_exp['HYBAS_ID'].apply(lambda x: str(x))\n",
    "bas_6_exp['PFAF_ID'] = bas_6_exp['PFAF_ID'].apply(lambda x: str(x))\n",
    "bas_6_exp['year'] = bas_6_exp['year'].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5eff63",
   "metadata": {},
   "source": [
    "# Conflicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e75cab52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>event_id_cnty</th>\n",
       "      <th>event_date</th>\n",
       "      <th>year</th>\n",
       "      <th>time_precision</th>\n",
       "      <th>disorder_type</th>\n",
       "      <th>event_type</th>\n",
       "      <th>sub_event_type</th>\n",
       "      <th>actor1</th>\n",
       "      <th>assoc_actor_1</th>\n",
       "      <th>inter1</th>\n",
       "      <th>...</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>geo_precision</th>\n",
       "      <th>source</th>\n",
       "      <th>source_scale</th>\n",
       "      <th>notes</th>\n",
       "      <th>fatalities</th>\n",
       "      <th>tags</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BFO10686</td>\n",
       "      <td>30 September 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Battles</td>\n",
       "      <td>Armed clash</td>\n",
       "      <td>JNIM: Group for Support of Islam and Muslims</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>13.8478</td>\n",
       "      <td>-2.4172</td>\n",
       "      <td>1</td>\n",
       "      <td>Whatsapp</td>\n",
       "      <td>New media</td>\n",
       "      <td>On 30 September 2023, presumed JNIM militants ...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1696869398</td>\n",
       "      <td>POINT (-2.41720 13.84780)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BFO10693</td>\n",
       "      <td>30 September 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Battles</td>\n",
       "      <td>Armed clash</td>\n",
       "      <td>JNIM: Group for Support of Islam and Muslims</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>12.7301</td>\n",
       "      <td>-4.0967</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook; Signal; Whatsapp</td>\n",
       "      <td>New media</td>\n",
       "      <td>On 30 September 2023, presumed JNIM militants ...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1696869398</td>\n",
       "      <td>POINT (-4.09670 12.73010)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>BFO10708</td>\n",
       "      <td>30 September 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Violence against civilians</td>\n",
       "      <td>Attack</td>\n",
       "      <td>JNIM: Group for Support of Islam and Muslims</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>13.7602</td>\n",
       "      <td>-2.426</td>\n",
       "      <td>1</td>\n",
       "      <td>Facebook</td>\n",
       "      <td>New media</td>\n",
       "      <td>On 30 September 2023, presumed JNIM militants ...</td>\n",
       "      <td>3</td>\n",
       "      <td></td>\n",
       "      <td>1696869398</td>\n",
       "      <td>POINT (-2.42600 13.76020)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CAO7889</td>\n",
       "      <td>30 September 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>2</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Violence against civilians</td>\n",
       "      <td>Attack</td>\n",
       "      <td>Islamic State (West Africa) and/or Boko Haram ...</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10.7424</td>\n",
       "      <td>13.8023</td>\n",
       "      <td>2</td>\n",
       "      <td>Twitter; Xinhua</td>\n",
       "      <td>New media-International</td>\n",
       "      <td>Around 30 September 2023, ISWAP or Boko Haram ...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1696869398</td>\n",
       "      <td>POINT (13.80230 10.74240)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CDI2876</td>\n",
       "      <td>30 September 2023</td>\n",
       "      <td>2023</td>\n",
       "      <td>1</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Violence against civilians</td>\n",
       "      <td>Attack</td>\n",
       "      <td>Police Forces of the Ivory Coast (2011-)</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.4125</td>\n",
       "      <td>-7.5538</td>\n",
       "      <td>1</td>\n",
       "      <td>Koaci News; Soir Info</td>\n",
       "      <td>National</td>\n",
       "      <td>On 30 September 2023, policemen beat a civilia...</td>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>1696869398</td>\n",
       "      <td>POINT (-7.55380 7.41250)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984432</th>\n",
       "      <td>SIE2</td>\n",
       "      <td>01 January 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Battles</td>\n",
       "      <td>Government regains territory</td>\n",
       "      <td>Military Forces of Sierra Leone (1996-1997)</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>8.4642</td>\n",
       "      <td>-10.9332</td>\n",
       "      <td>2</td>\n",
       "      <td>No Peace Without Justice; SL-LED</td>\n",
       "      <td>Local partner-New media</td>\n",
       "      <td>Around 1 January 1997 (month of), Military For...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1670286851</td>\n",
       "      <td>POINT (-10.93320 8.46420)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984433</th>\n",
       "      <td>SIE3</td>\n",
       "      <td>01 January 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Battles</td>\n",
       "      <td>Armed clash</td>\n",
       "      <td>Kamajor Militia</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>8.1221</td>\n",
       "      <td>-11.7047</td>\n",
       "      <td>2</td>\n",
       "      <td>No Peace Without Justice; SL-LED</td>\n",
       "      <td>Local partner-New media</td>\n",
       "      <td>Around 1 January 1997 (month of), Kamajor Mili...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1670286851</td>\n",
       "      <td>POINT (-11.70470 8.12210)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984434</th>\n",
       "      <td>SIE6</td>\n",
       "      <td>01 January 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Violence against civilians</td>\n",
       "      <td>Attack</td>\n",
       "      <td>Military Forces of Sierra Leone (1996-1997)</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>7.5317</td>\n",
       "      <td>-12.4694</td>\n",
       "      <td>2</td>\n",
       "      <td>No Peace Without Justice; SL-LED</td>\n",
       "      <td>Local partner-New media</td>\n",
       "      <td>Around 1 January 1997 (month of), Military For...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1670286851</td>\n",
       "      <td>POINT (-12.46940 7.53170)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984435</th>\n",
       "      <td>SIE7</td>\n",
       "      <td>01 January 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Battles</td>\n",
       "      <td>Armed clash</td>\n",
       "      <td>Kamajor Militia</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>8.231</td>\n",
       "      <td>-12.338</td>\n",
       "      <td>2</td>\n",
       "      <td>SL-LED; No Peace Without Justice</td>\n",
       "      <td>Local partner-New media</td>\n",
       "      <td>Around 1 January 1997 (month of), Kamajor Mili...</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>1670286851</td>\n",
       "      <td>POINT (-12.33800 8.23100)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984436</th>\n",
       "      <td>ANG2</td>\n",
       "      <td>01 January 1997</td>\n",
       "      <td>1997</td>\n",
       "      <td>3</td>\n",
       "      <td>Political violence</td>\n",
       "      <td>Violence against civilians</td>\n",
       "      <td>Attack</td>\n",
       "      <td>UNITA: National Union for the Total Independen...</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>-12.3833</td>\n",
       "      <td>16.9333</td>\n",
       "      <td>3</td>\n",
       "      <td>Diario de Noticias (Angola)</td>\n",
       "      <td>National</td>\n",
       "      <td>The total number of chiefs in the past 6 month...</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>1673316822</td>\n",
       "      <td>POINT (16.93330 -12.38330)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>984437 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       event_id_cnty         event_date  year time_precision  \\\n",
       "0           BFO10686  30 September 2023  2023              1   \n",
       "1           BFO10693  30 September 2023  2023              1   \n",
       "2           BFO10708  30 September 2023  2023              1   \n",
       "3            CAO7889  30 September 2023  2023              2   \n",
       "4            CDI2876  30 September 2023  2023              1   \n",
       "...              ...                ...   ...            ...   \n",
       "984432          SIE2    01 January 1997  1997              3   \n",
       "984433          SIE3    01 January 1997  1997              3   \n",
       "984434          SIE6    01 January 1997  1997              3   \n",
       "984435          SIE7    01 January 1997  1997              3   \n",
       "984436          ANG2    01 January 1997  1997              3   \n",
       "\n",
       "             disorder_type                  event_type  \\\n",
       "0       Political violence                     Battles   \n",
       "1       Political violence                     Battles   \n",
       "2       Political violence  Violence against civilians   \n",
       "3       Political violence  Violence against civilians   \n",
       "4       Political violence  Violence against civilians   \n",
       "...                    ...                         ...   \n",
       "984432  Political violence                     Battles   \n",
       "984433  Political violence                     Battles   \n",
       "984434  Political violence  Violence against civilians   \n",
       "984435  Political violence                     Battles   \n",
       "984436  Political violence  Violence against civilians   \n",
       "\n",
       "                      sub_event_type  \\\n",
       "0                        Armed clash   \n",
       "1                        Armed clash   \n",
       "2                             Attack   \n",
       "3                             Attack   \n",
       "4                             Attack   \n",
       "...                              ...   \n",
       "984432  Government regains territory   \n",
       "984433                   Armed clash   \n",
       "984434                        Attack   \n",
       "984435                   Armed clash   \n",
       "984436                        Attack   \n",
       "\n",
       "                                                   actor1 assoc_actor_1  \\\n",
       "0            JNIM: Group for Support of Islam and Muslims                 \n",
       "1            JNIM: Group for Support of Islam and Muslims                 \n",
       "2            JNIM: Group for Support of Islam and Muslims                 \n",
       "3       Islamic State (West Africa) and/or Boko Haram ...                 \n",
       "4                Police Forces of the Ivory Coast (2011-)                 \n",
       "...                                                   ...           ...   \n",
       "984432        Military Forces of Sierra Leone (1996-1997)                 \n",
       "984433                                    Kamajor Militia                 \n",
       "984434        Military Forces of Sierra Leone (1996-1997)                 \n",
       "984435                                    Kamajor Militia                 \n",
       "984436  UNITA: National Union for the Total Independen...                 \n",
       "\n",
       "       inter1  ...  latitude longitude geo_precision  \\\n",
       "0           2  ...   13.8478   -2.4172             1   \n",
       "1           2  ...   12.7301   -4.0967             1   \n",
       "2           2  ...   13.7602    -2.426             1   \n",
       "3           2  ...   10.7424   13.8023             2   \n",
       "4           1  ...    7.4125   -7.5538             1   \n",
       "...       ...  ...       ...       ...           ...   \n",
       "984432      1  ...    8.4642  -10.9332             2   \n",
       "984433      3  ...    8.1221  -11.7047             2   \n",
       "984434      1  ...    7.5317  -12.4694             2   \n",
       "984435      3  ...     8.231   -12.338             2   \n",
       "984436      2  ...  -12.3833   16.9333             3   \n",
       "\n",
       "                                  source             source_scale  \\\n",
       "0                               Whatsapp                New media   \n",
       "1             Facebook; Signal; Whatsapp                New media   \n",
       "2                               Facebook                New media   \n",
       "3                        Twitter; Xinhua  New media-International   \n",
       "4                  Koaci News; Soir Info                 National   \n",
       "...                                  ...                      ...   \n",
       "984432  No Peace Without Justice; SL-LED  Local partner-New media   \n",
       "984433  No Peace Without Justice; SL-LED  Local partner-New media   \n",
       "984434  No Peace Without Justice; SL-LED  Local partner-New media   \n",
       "984435  SL-LED; No Peace Without Justice  Local partner-New media   \n",
       "984436       Diario de Noticias (Angola)                 National   \n",
       "\n",
       "                                                    notes fatalities tags  \\\n",
       "0       On 30 September 2023, presumed JNIM militants ...          0        \n",
       "1       On 30 September 2023, presumed JNIM militants ...          0        \n",
       "2       On 30 September 2023, presumed JNIM militants ...          3        \n",
       "3       Around 30 September 2023, ISWAP or Boko Haram ...          1        \n",
       "4       On 30 September 2023, policemen beat a civilia...          1        \n",
       "...                                                   ...        ...  ...   \n",
       "984432  Around 1 January 1997 (month of), Military For...          0        \n",
       "984433  Around 1 January 1997 (month of), Kamajor Mili...          0        \n",
       "984434  Around 1 January 1997 (month of), Military For...          0        \n",
       "984435  Around 1 January 1997 (month of), Kamajor Mili...          0        \n",
       "984436  The total number of chiefs in the past 6 month...         10        \n",
       "\n",
       "         timestamp                    geometry  \n",
       "0       1696869398   POINT (-2.41720 13.84780)  \n",
       "1       1696869398   POINT (-4.09670 12.73010)  \n",
       "2       1696869398   POINT (-2.42600 13.76020)  \n",
       "3       1696869398   POINT (13.80230 10.74240)  \n",
       "4       1696869398    POINT (-7.55380 7.41250)  \n",
       "...            ...                         ...  \n",
       "984432  1670286851   POINT (-10.93320 8.46420)  \n",
       "984433  1670286851   POINT (-11.70470 8.12210)  \n",
       "984434  1670286851   POINT (-12.46940 7.53170)  \n",
       "984435  1670286851   POINT (-12.33800 8.23100)  \n",
       "984436  1673316822  POINT (16.93330 -12.38330)  \n",
       "\n",
       "[984437 rows x 32 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CRS of df is EPSG:3857\n",
    "df = gpd.read_file(conflict_path + '1997-01-01-2023-09-30.csv', sep=';', dtype={'timestamp': 'object'})\n",
    "gdf_conf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.longitude, df.latitude))\n",
    "gdf_conf.set_crs('epsg:4326', inplace=True)\n",
    "# gdf_conf.set_crs('epsg:3857', inplace=True)\n",
    "# gdf_conf.to_crs(epsg=4326, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d2d8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by region:\n",
    "regions = ['Eastern Africa', 'Middle Africa', 'Northern Africa', 'Southern Africa', 'Western Africa']\n",
    "gdf_conf = gdf_conf[gdf_conf['region'].apply(lambda x: True if x in regions else False)]\n",
    "gdf_conf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter by battles:\n",
    "battles_gdf = gdf_conf[gdf_conf.event_type == 'Battles']\n",
    "battles_gdf.reset_index(drop=True, inplace=True)\n",
    "battles_gdf = gdf_conf.copy()\n",
    "\n",
    "del gdf_conf\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc009a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match geography of conflicts with hydrobasins:\n",
    "joined = gpd.sjoin(bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']],\n",
    "                   battles_gdf[['event_id_cnty', 'event_date', 'geometry', 'year', 'fatalities', 'timestamp']], how='right')\n",
    "joined.drop(['index_left'], axis=1, inplace=True)\n",
    "joined[['day', 'month', 'year']] = joined['event_date'].str.split(' ', expand = True)\n",
    "joined['month_year'] = joined[['month', 'year']].apply(lambda x: ' '.join(x), axis=1)\n",
    "joined['month_year'] = joined['month_year'].apply(lambda x: datetime.strptime(x, '%B %Y'))\n",
    "joined['month'] = joined['month_year'].apply(lambda x: x.month)\n",
    "joined['year'] = joined['month_year'].apply(lambda x: x.year)\n",
    "\n",
    "# Create dict of lists of conflicts for every combination with of basin ID, month and year:\n",
    "basin_conf_dict = {}\n",
    "# for hydrobasin_id, confs_in_basin in joined.groupby(['HYBAS_ID', 'month', 'year']):\n",
    "for hydrobasin_id, confs_in_basin in joined.groupby(['HYBAS_ID', 'year']):\n",
    "    confs_ids = confs_in_basin['event_id_cnty'].tolist()\n",
    "    basin_conf_dict[hydrobasin_id] = confs_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6a05645c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99998882369376924646\r"
     ]
    }
   ],
   "source": [
    "# Match number of conflicts with basins, country and year:\n",
    "bas_6_exp['battles_per_y_loop'] = 0\n",
    "for idx, row in bas_6_exp.iterrows():\n",
    "    print(idx/len(bas_6_exp), end='\\r')\n",
    "    # key = (row['HYBAS_ID'], row['month'], row['year'])\n",
    "    key = (row['HYBAS_ID'], row['year'])\n",
    "    if key in basin_conf_dict.keys():\n",
    "        bas_6_exp.loc[idx, 'battles_per_y_loop'] = int(len(basin_conf_dict[key]))\n",
    "    else:\n",
    "        bas_6_exp.loc[idx, 'battles_per_y_loop'] = int(0)\n",
    "        \n",
    "# bas_6_exp.battles_per_m_loop.value_counts()\n",
    "bas_6_exp['had_fight'] = bas_6_exp['battles_per_y_loop'].apply(lambda x: 1 if x > 0 else 0)\n",
    "bas_6_exp['RGxD_hat'] = bas_6_exp['RG']*bas_6_exp['dams_in_c_per_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da1c5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6_exp.to_file(output_path + '5rivers_prepared_data.shp')\n",
    "bas_6_exp = gpd.read_file(output_path + '5rivers_prepared_data.shp')\n",
    "bas_6_exp.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist',\n",
    "                      'dams_in_c_':'dams_in_c_per_y', 'dams_per_c':'dams_per_c_99',\n",
    "                      'battles_pe':'battles_per_y_loop'}, inplace=True)\n",
    "# In case of weird column: dams_per_1 = dams_per_c_99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dams as endogenous variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_use = 1999\n",
    "dams_df = gpd.read_file(dams_path + 'GRanD_dams_v1_3.shp')\n",
    "\n",
    "# # Wherever main year is -99, so is alternative year:\n",
    "dams_df = dams_df[dams_df['YEAR'] != -99]\n",
    "dams_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "uses = {'Main', 'Major'}\n",
    "# uses = {'Main', 'Sec', 'Major'}\n",
    "\n",
    "dams_df['el_ir'] = dams_df.apply(lambda row: 1 if (row.USE_ELEC in uses) or \n",
    "                                 (row.USE_IRRI in uses) else 0, axis=1)\n",
    "dams_df = dams_df[dams_df['el_ir'] == 1]\n",
    "dams_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dams_1900_1999 = dams_df[(dams_df['YEAR'] >= 1900) & (dams_df['YEAR'] < year_to_use)].copy()\n",
    "dams_1900_1999.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dams_after_99 = dams_df[dams_df['YEAR'] >= year_to_use].copy()\n",
    "dams_after_99.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the amount of dams per basin by 1999:\n",
    "joined_1900_1999 = gpd.sjoin(dams_1900_1999[['GRAND_ID', 'YEAR', 'MAIN_USE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "joined_1900_1999 = joined_1900_1999[~joined_1900_1999['HYBAS_ID'].isna()]\n",
    "joined_1900_1999.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dam_bas_1900_99 = {}\n",
    "for bas_id, data in joined_1900_1999.groupby('HYBAS_ID'):\n",
    "    dam_ids = data['GRAND_ID'].to_list()\n",
    "    dam_bas_1900_99[bas_id] = dam_ids\n",
    "\n",
    "# Fill number of dams by 1999 for each basin with created dictionary values:\n",
    "bas_6_exp['basdams_99'] = bas_6_exp['HYBAS_ID'].apply(lambda x: len(dam_bas_1900_99[x]) if \n",
    "                                                      x in dam_bas_1900_99.keys() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the amount of dams per basin per year after 1999:\n",
    "joined_after_99 = gpd.sjoin(dams_after_99[['GRAND_ID', 'YEAR', 'MAIN_USE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "joined_after_99 = joined_after_99[~joined_after_99['HYBAS_ID'].isna()]\n",
    "joined_after_99.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dam_bas_after_99 = {}\n",
    "for bas_id, data in joined_after_99.groupby(['HYBAS_ID', 'YEAR']):\n",
    "    dam_ids = data['GRAND_ID'].to_list()\n",
    "    dam_bas_after_99[bas_id] = dam_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed_dams = {1987: [1918],\n",
    "#                 2002: [2085],\n",
    "#                 2003: [769],\n",
    "#                 2005: [2006],\n",
    "#                 2007: [820, 2844],\n",
    "#                 2008: [772, 2882],\n",
    "#                 2013: [6315],\n",
    "#                 2016: [6285]}\n",
    "\n",
    "# dams_df[dams_df.REM_YEAR != -99].COUNTRY.value_counts()\n",
    "\n",
    "# dam_bas_after_99\n",
    "# dam_bas_1900_99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99998882369376924646\r"
     ]
    }
   ],
   "source": [
    "# Create the dict of total amount of dams in basin c for year t:\n",
    "basins = set(bas_6_exp.HYBAS_ID)\n",
    "tot_dams_per_basin_year = {}\n",
    "idx = 0\n",
    "for basin in basins:\n",
    "    idx += 1\n",
    "    print(idx/len(basins), end='\\r')\n",
    "    tot_dams_per_year = {}\n",
    "    for year in range(1999, 2024):\n",
    "        if year == 1999:\n",
    "            if basin in dam_bas_1900_99.keys():\n",
    "                tot_dams_per_year[year] = len(dam_bas_1900_99[basin])\n",
    "            else:\n",
    "                tot_dams_per_year[year] = 0\n",
    "        else:\n",
    "            key = (basin, year)\n",
    "            if key in dam_bas_after_99.keys():\n",
    "                tot_dams_per_year[year] = tot_dams_per_year[year-1] + len(dam_bas_after_99[key])\n",
    "            else:\n",
    "                tot_dams_per_year[year] = tot_dams_per_year[year-1]\n",
    "    tot_dams_per_basin_year[basin] = tot_dams_per_year\n",
    "    \n",
    "# Use the created dictionary to fill in the basins dataframe:\n",
    "bas_6_exp['dams_in_b_per_y'] = 0\n",
    "for idx, row in bas_6_exp.iterrows():\n",
    "    print(idx/len(bas_6_exp), end='\\r')\n",
    "    bas_6_exp.loc[idx, 'dams_in_b_per_y'] = tot_dams_per_basin_year[row['HYBAS_ID']][row['year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_more_6\n",
      "tot_riv_dist\n",
      "dams_per_c_99\n",
      "dams_in_c_per_y\n",
      "battles_per_y_loop\n",
      "dams_in_b_per_y\n"
     ]
    }
   ],
   "source": [
    "# bas_6_exp.drop(columns=['basdams_py'], inplace=True)\n",
    "\n",
    "col_names = list(bas_6_exp.columns)\n",
    "for col in col_names:\n",
    "    if len(col) > 10:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_rename = {'grad_more_6': 'grad_m_6',\n",
    "                  'tot_riv_dist': 'tot_riv_di',\n",
    "                  'dams_per_c_99': 'dams_pc_99',\n",
    "                  'dams_in_b_per_y': 'dams_pby',\n",
    "                  'dams_in_c_per_y': 'dams_pcy',\n",
    "                  'battles_per_y_loop': 'btl_p_y'}\n",
    "\n",
    "bas_6_exp.rename(columns=cols_to_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HYBAS_ID', 'NEXT_DOWN', 'NEXT_SINK', 'MAIN_BAS', 'DIST_SINK',\n",
       "       'DIST_MAIN', 'SUB_AREA', 'UP_AREA', 'PFAF_ID', 'ENDO', 'COAST', 'ORDER',\n",
       "       'SORT', 'RG', 'grad_15_3', 'grad_3_6', 'grad_m_6', 'tot_riv_di',\n",
       "       'av_elev', 'shr_l_25', 'shr_25_50', 'shr_50_1k', 'shr_m_1k', 'Country',\n",
       "       'year', 'dams_pc_99', 'dams_per_1', 'dams_pcy', 'btl_p_y', 'had_fight',\n",
       "       'RGxD_hat', 'geometry', 'basdams_99', 'dams_pby'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bas_6_exp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction between geographic controls and D_hat\n",
    "bas_6_exp['z_ieygg'] = bas_6_exp['grad_15_3']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_tkdgw'] = bas_6_exp['grad_3_6']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_sobhm'] = bas_6_exp['grad_m_6']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_fobki'] = bas_6_exp['tot_riv_di']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_hyeah'] = bas_6_exp['av_elev']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_vcjei'] = bas_6_exp['shr_l_25']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_nlvsk'] = bas_6_exp['shr_25_50']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_ahjvn'] = bas_6_exp['shr_50_1k']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_zgjij'] = bas_6_exp['shr_m_1k']*bas_6_exp['dams_pcy']\n",
    "\n",
    "columns_names_meanings = {\n",
    "    'z_ieygg': 'Share of river gradient between 1,5% and 3% in a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_tkdgw': 'Share of river gradient between 3% and 6% in a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_sobhm': 'Share of river gradient more than 6% in a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_fobki': 'Total distance of rivers in a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_hyeah': 'Average elevation of a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_vcjei': 'Area of elevation lower than 250m as a share of basin area, multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_nlvsk': 'Area of elevation between 250m and 500m as a share of basin area, multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_ahjvn': 'Area of elevation between 500m and 1000m as a share of basin area, multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_zgjij': 'Area of elevation between 500m and 1000m as a share of basin area, multiplied by total number of dams in a corresponding country in a corresponding year.'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6_exp.to_file(output_path + '5rivers_fully_prepared_data_all_conflicts.shp')\n",
    "bas_6 = gpd.read_file(output_path + '5rivers_fully_prepared_data_all_conflicts.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_6[\"centroid\"] = bas_6[\"geometry\"].centroid\n",
    "bas_6[\"lon\"] = bas_6[\"centroid\"].apply(lambda p: p.x)\n",
    "bas_6[\"lat\"] = bas_6[\"centroid\"].apply(lambda p: p.y)\n",
    "bas_6.drop(columns=['centroid'], inplace= True)\n",
    "bas_6.to_file(output_path + '5rivers_fully_prepared_data_all_conflicts_centroid.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
