{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e33a6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a36801",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from math import radians, cos, sin, asin, sqrt, atan2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import networkx as nx\n",
    "\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from pyproj import Proj, transform\n",
    "\n",
    "import rasterio\n",
    "from rasterio.transform import xy\n",
    "from rasterio.plot import show\n",
    "from rasterio.features import geometry_mask\n",
    "\n",
    "from shapely.geometry import Point, Polygon, LineString, MultiLineString\n",
    "\n",
    "import statsmodels.formula.api as smf\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e7b109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/gleb/Desktop/thesis/data/'\n",
    "\n",
    "countries_path = data_path + 'world-administrative-boundaries/'\n",
    "basins_6_path = data_path + 'HydroBASINS Africa 6 level/'\n",
    "basins_12_path = data_path + 'HydroBASINS Africa 12 level/'\n",
    "conflict_path = data_path + 'conflicts/'\n",
    "dams_path = data_path + 'dams/'\n",
    "sheds_file_3 = data_path + 'hyd_af_dem_3s/af_dem_3s.tif'\n",
    "sheds_file_15 = data_path + 'hyd_af_dem_15s/hyd_af_dem_15s.tif'\n",
    "rivers_file = data_path + 'HydroRIVERS_v10_af/HydroRIVERS_v10_af.gdb'\n",
    "\n",
    "output_path = '/home/gleb/Desktop/thesis/outcomes/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c6e9428",
   "metadata": {},
   "outputs": [],
   "source": [
    "rivers = gpd.read_file(rivers_file)\n",
    "levels = [1,2,3,4,5]\n",
    "rivers_main = rivers[rivers['ORD_CLAS'].apply(lambda x: True if x in levels else False)].copy()\n",
    "rivers_main.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0789ac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with rasterio.open(sheds_file_15) as dem:\n",
    "    dem_data = dem.read(1)\n",
    "    transformation = dem.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22ebe701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_riv_gradient(elevation_data, transform, line_string):\n",
    "    points = [xy for xy in line_string.coords]\n",
    "    gradient_values = []\n",
    "    distance_values = []\n",
    "    for idx in range(len(points) - 1):\n",
    "        point1, point2 = points[idx], points[idx + 1]\n",
    "        col1, row1 = rasterio.transform.rowcol(transformation, point1[0], point1[1])\n",
    "        col2, row2 = rasterio.transform.rowcol(transformation, point2[0], point2[1])\n",
    "        elevation1 = dem_data[col1, row1]\n",
    "#         print('Elevation level of point 1: ' + str(elevation1))\n",
    "        elevation2 = dem_data[col2, row2]\n",
    "#         print('Elevation level of point 2: ' + str(elevation2))\n",
    "        distance = haversine(point1[0], point1[1], point2[0], point2[1])\n",
    "#         print('Distance between points: ' + str(distance))\n",
    "        gradient = ((elevation1 - elevation2) / distance)*100\n",
    "#         print('Gradient between points: ' + str(gradient))\n",
    "        distance_values.append(distance)\n",
    "        gradient_values.append(gradient)\n",
    "    return gradient_values, distance_values\n",
    "\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 * 1000\n",
    "    return c * r\n",
    "\n",
    "def gradient_fits(grad, category='suitable'):\n",
    "    if category=='suitable':\n",
    "        if (grad >= 1.5) & (grad <= 3):\n",
    "            return True\n",
    "        elif grad >= 6:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif category=='1.5-3':\n",
    "        if (grad >= 1.5) & (grad <= 3):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif category == '3-6':\n",
    "        if (grad > 3) & (grad < 6):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    elif category == '>6':\n",
    "        if grad >= 6:\n",
    "            return True \n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2913da3",
   "metadata": {},
   "source": [
    "# Test for river distances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e6c2d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7221.239512363846"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if isinstance(river_geoms, MultiLineString):\n",
    "#     grads, dists = calculate_gradient(dem_data, transformation, river_geoms[0])\n",
    "index = 24740\n",
    "river_geoms = rivers_main.loc[index, 'geometry']\n",
    "grads, dists = calculate_riv_gradient(dem_data, transformation, river_geoms.geoms[0])\n",
    "sum(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5228315b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HYRIV_ID</th>\n",
       "      <th>NEXT_DOWN</th>\n",
       "      <th>MAIN_RIV</th>\n",
       "      <th>LENGTH_KM</th>\n",
       "      <th>DIST_DN_KM</th>\n",
       "      <th>DIST_UP_KM</th>\n",
       "      <th>CATCH_SKM</th>\n",
       "      <th>UPLAND_SKM</th>\n",
       "      <th>ENDORHEIC</th>\n",
       "      <th>DIS_AV_CMS</th>\n",
       "      <th>ORD_STRA</th>\n",
       "      <th>ORD_CLAS</th>\n",
       "      <th>ORD_FLOW</th>\n",
       "      <th>HYBAS_L12</th>\n",
       "      <th>Shape_Length</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>24740</th>\n",
       "      <td>10025303</td>\n",
       "      <td>10024963</td>\n",
       "      <td>10018841</td>\n",
       "      <td>7.22</td>\n",
       "      <td>137.100006</td>\n",
       "      <td>11.1</td>\n",
       "      <td>25.48</td>\n",
       "      <td>25.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1.120109e+09</td>\n",
       "      <td>0.07092</td>\n",
       "      <td>MULTILINESTRING ((-6.55833 33.26250, -6.55208 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       HYRIV_ID  NEXT_DOWN  MAIN_RIV  LENGTH_KM  DIST_DN_KM  DIST_UP_KM  \\\n",
       "24740  10025303   10024963  10018841       7.22  137.100006        11.1   \n",
       "\n",
       "       CATCH_SKM  UPLAND_SKM  ENDORHEIC  DIS_AV_CMS  ORD_STRA  ORD_CLAS  \\\n",
       "24740      25.48        25.5          0       0.047         1         2   \n",
       "\n",
       "       ORD_FLOW     HYBAS_L12  Shape_Length  \\\n",
       "24740         8  1.120109e+09       0.07092   \n",
       "\n",
       "                                                geometry  \n",
       "24740  MULTILINESTRING ((-6.55833 33.26250, -6.55208 ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rivers_main.iloc[index:index+1, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9b60a",
   "metadata": {},
   "source": [
    "# Basins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17c4a9e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bas_6 = gpd.read_file(basins_6_path + 'hybas_af_lev06_v1c.shp')\n",
    "bas_12 = gpd.read_file(basins_12_path + 'hybas_af_lev12_v1c.shp')\n",
    "bas_12['PFAF_ID_6l'] = bas_12['PFAF_ID'].apply(lambda x: int(str(x)[:6]))\n",
    "bas_l12_to_6 = pd.Series(bas_12.PFAF_ID_6l.values,index=bas_12.HYBAS_ID).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570f3e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate IV and geographic controls for each basin: \n",
    "# 1. Revir Gradient = ratio_of_grad, check\n",
    "# 2. basin size = SUB_AREA (in sq.km), check\n",
    "# 3. elevation = average_elevation, check\n",
    "# 4. average basin gradient = average_gradient, check\n",
    "# 5. river length= tot_riv_dist, check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b87b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching rivers to basins of 6th level using PFAF ids:\n",
    "rivers['HYBAS_L6_PFAFID'] = rivers['HYBAS_L12'].apply(lambda x: bas_l12_to_6[x])\n",
    "rivers_main['HYBAS_L6_PFAFID'] = rivers_main['HYBAS_L12'].apply(lambda x: bas_l12_to_6[x])\n",
    "\n",
    "# Dictionary of rivers geometries. Keys: rivers' HYRIV_ID. Values: geometries.\n",
    "riv_geoms = pd.Series(rivers.geometry.values,index=rivers.HYRIV_ID).to_dict()\n",
    "\n",
    "# Dictionary basins-rivers. Keys: Basin's PFAF index. Values: lists of 'HYRIV_ID' that belong to a basin.\n",
    "bas_riv_dict = rivers_main.groupby('HYBAS_L6_PFAFID')['HYRIV_ID'].apply(list).to_dict()\n",
    "# bas_riv_dict = rivers.groupby('HYBAS_L6_PFAFID')['HYRIV_ID'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35c9563b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated ratio of IDs: 0.99972199054767864543\r"
     ]
    }
   ],
   "source": [
    "# Calculate ratios of suitable river gradients and total length of rivers for basins:\n",
    "ratios_RG = {}\n",
    "ratios_15_3 = {}\n",
    "ratios_3_6 = {}\n",
    "ratios_more_6 = {}\n",
    "basin_riv_dist = {}\n",
    "\n",
    "for pfaf_id in set(bas_6.PFAF_ID):\n",
    "    print('Calculated ratio of IDs: ' + str(len(ratios_RG)/len(set(bas_6.PFAF_ID))), end = '\\r')\n",
    "    if pfaf_id in bas_riv_dict.keys():\n",
    "        rivers_in_basin = bas_riv_dict[pfaf_id]\n",
    "        \n",
    "        total_rivers_distance = 0\n",
    "        fitted_gradients_distance_RG = 0\n",
    "        fitted_gradients_distance_15_3 = 0\n",
    "        fitted_gradients_distance_3_6 = 0\n",
    "        fitted_gradients_distance_more_6 = 0\n",
    "        \n",
    "        for riv in rivers_in_basin:\n",
    "            river_geom = riv_geoms[riv]\n",
    "            grads, dists = calculate_riv_gradient(dem_data, transformation, river_geom.geoms[0])\n",
    "            # Which sections of the river have gradient 1.5-3% or >3%:\n",
    "            grad_fits_RG = [gradient_fits(grad, category='suitable') for grad in grads]\n",
    "            grad_fits_15_3 = [gradient_fits(grad, category='1.5-3') for grad in grads]\n",
    "            grad_fits_3_6 = [gradient_fits(grad, category='3-6') for grad in grads]\n",
    "            grad_fits_more_6 = [gradient_fits(grad, category='>6') for grad in grads]\n",
    "            # Calculate distance of sections within needed gradient:\n",
    "            fitted_gradients_distance_RG += sum([x for x, y in zip(dists, grad_fits_RG) if y])\n",
    "            fitted_gradients_distance_15_3 += sum([x for x, y in zip(dists, grad_fits_15_3) if y])\n",
    "            fitted_gradients_distance_3_6 += sum([x for x, y in zip(dists, grad_fits_3_6) if y])\n",
    "            fitted_gradients_distance_more_6 += sum([x for x, y in zip(dists, grad_fits_more_6) if y])\n",
    "            # Add length of the river to the distance of all rivers in the basin:\n",
    "            total_rivers_distance += sum(dists)\n",
    "        ratio_RG = fitted_gradients_distance_RG/total_rivers_distance\n",
    "        ratio_15_3 = fitted_gradients_distance_15_3/total_rivers_distance\n",
    "        ratio_3_6 = fitted_gradients_distance_3_6/total_rivers_distance\n",
    "        ratio_more_6 = fitted_gradients_distance_more_6/total_rivers_distance\n",
    "        \n",
    "        ratios_RG[pfaf_id] = ratio_RG\n",
    "        ratios_15_3[pfaf_id] = ratio_15_3\n",
    "        ratios_3_6[pfaf_id] = ratio_3_6\n",
    "        ratios_more_6[pfaf_id] = ratio_more_6\n",
    "        basin_riv_dist[pfaf_id] = total_rivers_distance\n",
    "    else:\n",
    "        ratios_RG[pfaf_id] = 0\n",
    "        ratios_15_3[pfaf_id] = 0\n",
    "        ratios_3_6[pfaf_id] = 0\n",
    "        ratios_more_6[pfaf_id] = 0\n",
    "        basin_riv_dist[pfaf_id] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9af21197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6['RG'] = bas_6['grad_3_6'] \n",
    "bas_6['RG'] = bas_6['PFAF_ID'].apply(lambda x: ratios_RG[x])\n",
    "bas_6['grad_15_3'] = bas_6['PFAF_ID'].apply(lambda x: ratios_15_3[x])\n",
    "bas_6['grad_3_6'] = bas_6['PFAF_ID'].apply(lambda x: ratios_3_6[x])\n",
    "bas_6['grad_more_6'] = bas_6['PFAF_ID'].apply(lambda x: ratios_more_6[x])\n",
    "bas_6['tot_riv_dist'] = bas_6['PFAF_ID'].apply(lambda x: int(basin_riv_dist[x]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67057b8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# bas_6.to_file(output_path + 'hybas_af_lev06_v1c_grads.shp')\n",
    "bas_6 = gpd.read_file(output_path + 'hybas_af_lev06_v1c_grads.shp')\n",
    "bas_6.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ef05b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average elevation and gradient for basins:\n",
    "def calculate_bas_controls(geom, dem_file=dem_data):\n",
    "    mask = geometry_mask([geom], out_shape=dem_file.shape, transform=transformation,\n",
    "                         all_touched=False, invert=False)\n",
    "    # Calculate average elevation:\n",
    "    basin_elevations = np.ma.array(dem_data, mask=mask)\n",
    "    av_elevation = basin_elevations.mean()\n",
    "    basin_elevations = basin_elevations[~basin_elevations.mask].data\n",
    "    n_pixels = len(basin_elevations)\n",
    "    pxs_25 = len(basin_elevations[basin_elevations<=250])\n",
    "    pxs_25_50 = len(basin_elevations[(basin_elevations>250) & (basin_elevations<=500)])\n",
    "    pxs_50_1k = len(basin_elevations[(basin_elevations>500) & (basin_elevations<=1000)])\n",
    "    pxs_more_1k = len(basin_elevations[(basin_elevations>1000) & (basin_elevations<=10000)])\n",
    "    share_less_25 = pxs_25/n_pixels\n",
    "    share_25_50 = pxs_25_50/n_pixels\n",
    "    share_50_1k = pxs_50_1k/n_pixels\n",
    "    share_more_1k = pxs_more_1k/n_pixels\n",
    "    return [av_elevation, share_less_25, share_25_50, share_50_1k, share_more_1k]\n",
    "\n",
    "bas_6[['av_elev', 'shr_l_25', 'shr_25_50', 'shr_50_1k', 'shr_m_1k']] = pd.DataFrame(bas_6['geometry'].apply(\n",
    "    lambda x: calculate_bas_controls(x)).tolist(), index= bas_6.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1419e078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6.to_file(output_path + '5rivers_controls.shp')\n",
    "bas_6 = gpd.read_file(output_path + '5rivers_controls.shp')\n",
    "bas_6.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist'}, inplace=True)\n",
    "bas_6['HYBAS_ID'] = bas_6['HYBAS_ID'].apply(lambda x: str(x))\n",
    "bas_6['PFAF_ID'] = bas_6['PFAF_ID'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0711b112",
   "metadata": {},
   "source": [
    "# Compose data on dams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88e2eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dams and filter only dams for African continent\n",
    "dams_df = gpd.read_file(dams_path + 'GRanD_Version_1_3/GRanD_dams_v1_3.shp')\n",
    "countries = gpd.read_file(countries_path + 'world-administrative-boundaries.shp')\n",
    "regions = ['Eastern Africa', 'Middle Africa', 'Northern Africa', 'Southern Africa', 'Western Africa']\n",
    "countries = countries[countries['region'].apply(lambda x: True if x in regions else False)]\n",
    "countries_dict = {\"CÃ´te d'Ivoire\": 'Ivory Coast',\n",
    "                  'Democratic Republic of the Congo': 'Congo (DRC)',\n",
    "                  'Libyan Arab Jamahiriya': 'Libya',\n",
    "                  'United Republic of Tanzania':'Tanzania'}\n",
    "countries['name'] = countries['name'].apply(lambda x: countries_dict.get(x, x))\n",
    "countries = set(countries['name'])\n",
    "\n",
    "dams_df = dams_df[dams_df['COUNTRY'].apply(lambda x: True if x in countries else False)]\n",
    "dams_df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "802324dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install openpyxl\n",
    "\n",
    "# Add dams from FHReD 2015\n",
    "fhred = pd.read_excel(dams_path + 'FHReD_2015_future_dams_Zarfl_et_al_beta_version.xlsx', sheet_name = 1)\n",
    "fhred = fhred[(fhred['Continent'] == 'Africa')&(fhred['End'] <= 2023)].copy()\n",
    "fhred['DAM_ID'] = fhred['DAM_ID'] + 10000\n",
    "# Drop Continent, Capacity (MW), Stage, Start, Reference 1, Reference 2, Reference 3 columns\n",
    "fhred.drop(columns=['Continent', 'Capacity (MW)', 'Stage', 'Start', 'Reference 1', 'Reference 2', 'Reference 3'], inplace=True)\n",
    "cols_to_rename = {'DAM_ID':'GRAND_ID', 'Project name':'DAM_NAME', 'Country':'COUNTRY', 'Main_river':'RIVER', 'Major Basin':'MAIN_BASIN', 'Lon_Cleaned':'LONG_DD', 'LAT_cleaned':'LAT_DD', 'End':'YEAR'}\n",
    "fhred.rename(columns = cols_to_rename, inplace=True)\n",
    "fhred['USE_ELEC'] = 'Major'\n",
    "fhred['USE_IRRI'] = 'Major'\n",
    "fhred.reset_index(inplace=True, drop=True)\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(fhred['LONG_DD'], fhred['LAT_DD'])]\n",
    "fhred_gpd = gpd.GeoDataFrame(fhred, geometry=geometry)\n",
    "fhred_gpd.set_crs('epsg:4326', inplace=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07dc43b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "aqua = pd.read_excel(dams_path + 'Africa-dams_eng.xlsx', sheet_name = 1)\n",
    "cols_to_rename = {'Reservoir capacity (million m3)': 'CAP_MAX', 'Dam height (m)': 'DAM_HGT_M', 'Alternate dam name':'DAM_NAME', \n",
    "                  'Country':'COUNTRY', 'River':'RIVER', 'Major basin':'MAIN_BASIN',\n",
    "                  'Decimal degree longitude':'LONG_DD', 'Decimal degree latitude':'LAT_DD',\n",
    "                  'Completed /operational since':'YEAR'}\n",
    "aqua.rename(columns = cols_to_rename, inplace=True)\n",
    "aqua['USE_IRRI'] = aqua['Irrigation '].apply(lambda x: 'Major' if x == 'x' else None)\n",
    "aqua['USE_ELEC'] = aqua['Hydroelectricity (MW)'].apply(lambda x: 'Major' if x == 'x' else None)\n",
    "aqua['USE_SUPP'] = aqua['Water supply'].apply(lambda x: 'Major' if x == 'x' else None)\n",
    "aqua['USE_FCON'] = aqua['Flood control'].apply(lambda x: 'Major' if x == 'x' else None)\n",
    "aqua['USE_RECR'] = aqua['Recreation'].apply(lambda x: 'Major' if x == 'x' else None)\n",
    "aqua['USE_NAVI'] = aqua['Navigation'].apply(lambda x: 'Major' if x == 'x' else None)\n",
    "aqua['USE_PCON'] = aqua['Pollution control'].apply(lambda x: 'Major' if x == 'x' else None)\n",
    "aqua['USE_LIVE'] = aqua['Livestock rearing'].apply(lambda x: 'Major' if x == 'x' else None)\n",
    "aqua['USE_OTHR'] = aqua['Other'].apply(lambda x: 'Major' if x == 'x' else None)\n",
    "\n",
    "cols_to_remove = ['Name of dam', 'ISO alpha- 3', 'Administrative\\nUnit', 'Nearest city', 'Sub-basin', 'Reservoir area (km2)',\n",
    "                  'Sedimen-tation \\n(latest known) \\n(%) ', 'National reference(s)', 'Other reference(s)', 'Comments', 'Irrigation ',\n",
    "                  'Hydroelectricity (MW)','Water supply','Flood control','Recreation','Navigation','Pollution control','Livestock rearing','Other']\n",
    "aqua.drop(columns= cols_to_remove, inplace=True)\n",
    "\n",
    "aqua['YEAR'] = aqua['YEAR'].apply(lambda x: x if type(x) == int else None)\n",
    "aqua = aqua[~aqua['YEAR'].isna()].copy()\n",
    "aqua['YEAR'] = aqua['YEAR'].apply(lambda x: int(x))\n",
    "aqua = aqua[~aqua['LAT_DD'].isna()].copy()\n",
    "aqua.reset_index(inplace=True, drop=True)\n",
    "aqua['GRAND_ID'] = range(20001, 20001 + len(aqua))\n",
    "\n",
    "geometry = [Point(xy) for xy in zip(aqua['LONG_DD'], aqua['LAT_DD'])]\n",
    "aqua_gpd = gpd.GeoDataFrame(aqua, geometry=geometry)\n",
    "aqua_gpd.set_crs('epsg:4326', inplace=True)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dams_df = pd.concat([dams_df, fhred_gpd, aqua_gpd], ignore_index=True)\n",
    "dams_df = dams_df.sort_values(by='YEAR', ascending=False)\n",
    "dams_df.drop_duplicates(subset=['geometry'], keep='first', inplace=True)\n",
    "\n",
    "del aqua\n",
    "del fhred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90efa3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import itertools\n",
    "def haversine(lon1, lat1, lon2, lat2):\n",
    "    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n",
    "    dlon = lon2 - lon1 \n",
    "    dlat = lat2 - lat1 \n",
    "    a = sin(dlat/2)**2 + cos(lat1) * cos(lat2) * sin(dlon/2)**2\n",
    "    c = 2 * asin(sqrt(a)) \n",
    "    r = 6371 * 1000\n",
    "    return c * r\n",
    "\n",
    "# distance = haversine(point1[0], point1[1], point2[0], point2[1])\n",
    "\n",
    "dams_df = dams_df.sort_values(by='GRAND_ID', ascending=False)\n",
    "\n",
    "G = nx.Graph()\n",
    "for index, row in dams_df.iterrows():\n",
    "    # print(index)\n",
    "    # print('Lon: ' + str(row['LONG_DD']) + ', Lat: ' + str(row['LAT_DD']))\n",
    "    # print('X: ' + str(row.geometry.x) + ', Y: ' + str(row.geometry.x))\n",
    "    G.add_node(row['GRAND_ID'], year=row['YEAR'], pos=(row.geometry.x, row.geometry.y))\n",
    "\n",
    "for (id1, geom1, year1), (id2, geom2, year2) in itertools.combinations(dams_df[['GRAND_ID', 'geometry', 'YEAR']].values, 2):\n",
    "    # Calculate the distance in meters\n",
    "    # distance = haversine(lon1, lat1, lon2, lat2)\n",
    "    distance = haversine(geom1.x, geom1.y, geom2.x, geom2.y)\n",
    "    if distance > 1000:\n",
    "        continue\n",
    "    # Add an edge between the nodes with the distance as weight and years as attributes\n",
    "    G.add_edge(id1, id2, weight=distance, year1=year1, year2=year2)\n",
    "\n",
    "df = nx.to_pandas_edgelist(G, source='id1', target='id2')\n",
    "dams_to_remove = set(df['id1'])\n",
    "dams_df['duplicate'] = dams_df['GRAND_ID'].apply(lambda x: 1 if x in dams_to_remove else 0)\n",
    "dams_df = dams_df[dams_df['duplicate'] == 0]\n",
    "dams_df.reset_index(inplace=True, drop=True)\n",
    "dams_df.to_file(output_path + 'combined_dams.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ea5e2d",
   "metadata": {},
   "source": [
    "## Assign basins to the countries and vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d63c915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_temp = bas_6[['HYBAS_ID', 'geometry']].copy()\n",
    "# Create centroids of basins:\n",
    "bas_temp[\"geometry\"] = bas_temp[\"geometry\"].centroid\n",
    "# Read boundaries of countries:\n",
    "countries = gpd.read_file(countries_path + 'world-administrative-boundaries.shp')\n",
    "cntry_name_match = {'Bosnia & Herzegovina':'Bosnia and Herzegovina',\n",
    "                    'Democratic Republic of the Congo':'Congo (DRC)',\n",
    "                    'Iran (Islamic Republic of)':'Iran',\n",
    "                    \"CÃ´te d'Ivoire\":'Ivory Coast',\"Lao People's Democratic Republic\":'Laos',\n",
    "                    'Libyan Arab Jamahiriya':'Libya', \"Ma'tan al-Sarra\":'Libya',\n",
    "                    'The former Yugoslav Republic of Macedonia':'Macedonia',\n",
    "                    'Moldova, Republic of':'Moldova', 'Myanmar':'Myanmar (Burma)',\n",
    "                    \"Democratic People's Republic of Korea\": 'North Korea',\n",
    "                    'Russian Federation':'Russia', 'Republic of Korea':'South Korea',\n",
    "                    'Syrian Arab Republic':'Syria', 'United Republic of Tanzania':'Tanzania',\n",
    "                    'U.K. of Great Britain and Northern Ireland':'United Kingdom','United States of America':'United States'}\n",
    "\n",
    "countries['name'] = countries['name'].apply(lambda x: cntry_name_match[x] if x in cntry_name_match.keys() else x)\n",
    "afr_countries = countries[countries.continent == 'Africa'].copy()\n",
    "afr_countries.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Match centroids of basins with countries:\n",
    "joined = gpd.sjoin(bas_temp, afr_countries, how='left', op='within')\n",
    "joined = joined[~joined[['index_right']].isna().any(axis=1)].reset_index(drop=True)\n",
    "bas_country_dict = pd.Series(joined.name.values,index=joined.HYBAS_ID).to_dict()\n",
    "\n",
    "# Create lists of basins for each corresponding country:\n",
    "country_bas_dict = {}\n",
    "for country, basins_in_country in joined.groupby('name'):\n",
    "    bas_ids = basins_in_country['HYBAS_ID'].to_list()\n",
    "    country_bas_dict[country] = bas_ids\n",
    "    \n",
    "# Hold accounts of not assigned basins\n",
    "not_assigned_bas = set(joined[joined[['iso3']].isna().any(axis=1)]['HYBAS_ID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually add coastal basins for which centroid lies outside of the country borders:\n",
    "# Extend not assigned basins dict:\n",
    "not_assigned_bas = {'1060008110': 'Somalia',\n",
    "                    # '1060003780':,\n",
    "                    '1060020500': 'Gabon',\n",
    "                    '1060023020': 'Nigeria',\n",
    "                    '1060032860': 'Libyan Arab Jamahiriya',\n",
    "                    # '1060034490': 'Sao Tome and Principe',\n",
    "                    # '1060034610': ,\n",
    "                    # '1060034900': ,\n",
    "                    '1060035090': 'United Republic of Tanzania',\n",
    "                    # '1060040030': 'Madagascar',\n",
    "                    # '1060040040': 'Madagascar',\n",
    "                    # '1060040050': 'Comoros',\n",
    "                    # '1060040110': 'Seychelles',\n",
    "                    # '1060040140': 'Seychelles',\n",
    "                    # '1060040160': ,\n",
    "                    # '1060040180':\n",
    "                    }\n",
    "\n",
    "# Apply dicts to fill the countries for basins:\n",
    "bas_6['Country'] = bas_6['HYBAS_ID'].apply(lambda x: bas_country_dict.get(x, 'None'))\n",
    "bas_6['Country'] = bas_6['Country'].apply(lambda x: not_assigned_bas.get(x, x))\n",
    "\n",
    "# Drop basins w/o assigned countries (these basins have coastal and island nature): \n",
    "bas_6 = bas_6[bas_6['Country'] != 'None']\n",
    "bas_6.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust the disputed territories:\n",
    "country_adjust = {\"Hala'ib Triangle\": 'Egypt',\n",
    "                  'Ilemi Triangle': 'Kenya'}\n",
    "bas_6['Country'] = bas_6['Country'].apply(lambda x: country_adjust[x] if x in country_adjust.keys() else x)\n",
    "\n",
    "# # Following the identification strategy, I remove countries which occupy only 1 hydrobasin\n",
    "# Find countries with 1 hydrobasin:\n",
    "countries_to_remove = set()\n",
    "for country in country_bas_dict.keys():\n",
    "    if len(country_bas_dict[country]) == 1:\n",
    "        countries_to_remove.add(country)\n",
    "\n",
    "# {'Burundi', 'Djibouti', 'Ilemi Triangle'}\n",
    "\n",
    "bas_6 = bas_6[~bas_6['Country'].isin(countries_to_remove)].copy()\n",
    "bas_6.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6.to_file(output_path + '5rivers_all_controls.shp')\n",
    "bas_6 = gpd.read_file(output_path + '5rivers_all_controls.shp')\n",
    "bas_6.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist'}, inplace=True)\n",
    "bas_6['HYBAS_ID'] = bas_6['HYBAS_ID'].apply(lambda x: str(x))\n",
    "bas_6['PFAF_ID'] = bas_6['PFAF_ID'].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode the basin data frame by month (1-12) and year (1997-2023) to a panel dataset:\n",
    "bas_6['year'] = bas_6['HYBAS_ID'].apply(lambda x: [year for year in range(1999, 2024)])\n",
    "bas_6_exp = bas_6.explode('year', ignore_index=True)\n",
    "# bas_6_exp['month'] = bas_6_exp['HYBAS_ID'].apply(lambda x: [month for month in range(1, 13)])\n",
    "# bas_6_exp = bas_6_exp.explode('month', ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match dams per country per year with basins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # IDs of dams that were removed in corresponding years:\n",
    "# rem_dams = dams_df[dams_df['REM_YEAR'] != -99].copy().reset_index(drop=True)\n",
    "# # rem_dams = pd.Series(rem_dams.REM_YEAR.values,index=rem_dams.GRAND_ID).to_dict()\n",
    "# year_rem_dict = {}\n",
    "# for year, data in rem_dams.groupby('REM_YEAR'):\n",
    "#     dam_ids = data['GRAND_ID'].to_list()\n",
    "#     year_rem_dict[year] = dam_ids\n",
    "# year_rem_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f3d2bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the total number of dams in country c in year t:\n",
    "# Split the dams df:\n",
    "year_to_use = 1999\n",
    "# dams_df = gpd.read_file(dams_path + 'GRanD_dams_v1_3.shp')\n",
    "dams_df = gpd.read_file(output_path + 'combined_dams.shp')\n",
    "\n",
    "dams_before_start = dams_df[dams_df['YEAR'] < year_to_use]\n",
    "\n",
    "# Create a dict of countries and dams:\n",
    "dams_p_cntr_at_start = dams_before_start.groupby('COUNTRY').count().reset_index()\n",
    "dams_p_cntr_at_start = pd.Series(dams_p_cntr_at_start.GRAND_ID.values,\n",
    "                                index=dams_p_cntr_at_start.COUNTRY).to_dict()\n",
    "\n",
    "# Create column in for dams per country in 1999:\n",
    "bas_6_exp['dams_per_count_991'] = bas_6_exp['Country'].apply(lambda x: dams_p_cntr_at_start.get(x, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99998882369376924646\r"
     ]
    }
   ],
   "source": [
    "# # Calculate dams for country c for year t:\n",
    "# Split the df:\n",
    "dams_after_start = dams_df[dams_df['YEAR'] >= year_to_use]\n",
    "dams_after_start = dams_after_start.groupby(['COUNTRY', 'YEAR']).count().reset_index()\n",
    "\n",
    "# Create the dict where key is tuple of country and year:\n",
    "new_dams_after_start = dict()\n",
    "for index, row in dams_after_start.iterrows():\n",
    "    key = (row['COUNTRY'], row['YEAR'])\n",
    "    new_dams_after_start[key] = row['GRAND_ID']\n",
    "\n",
    "# Create the dict of total amount of dams in country c for year t:\n",
    "counts = set(bas_6_exp.Country)\n",
    "tot_dams_per_country_year = {}\n",
    "idx = 0\n",
    "for country in counts:\n",
    "    idx += 1\n",
    "    print(idx/len(counts), end='\\r')\n",
    "    tot_dams_per_year = {}\n",
    "    for year in range(1999, 2024):\n",
    "        if year == 1999:\n",
    "            if country in dams_p_cntr_at_start.keys():\n",
    "                tot_dams_per_year[year] = dams_p_cntr_at_start[country]\n",
    "            else:\n",
    "                tot_dams_per_year[year] = 0\n",
    "        else:\n",
    "            key = (country, year)\n",
    "            if key in new_dams_after_start.keys():\n",
    "                tot_dams_per_year[year] = tot_dams_per_year[year-1] + new_dams_after_start[key]\n",
    "            else:\n",
    "                tot_dams_per_year[year] = tot_dams_per_year[year-1]\n",
    "    tot_dams_per_country_year[country] = tot_dams_per_year\n",
    "    \n",
    "# Use the created dictionary to fill in the basins dataframe:\n",
    "bas_6_exp['n_of_dam_in_c_per_y'] = 0\n",
    "for idx, row in bas_6_exp.iterrows():\n",
    "    print(idx/len(bas_6_exp), end='\\r')\n",
    "    bas_6_exp.loc[idx, 'n_of_dam_in_c_per_y'] = tot_dams_per_country_year[row['Country']][row['year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6_exp.to_file(output_path + '5rivers_all_controls_panel.shp')\n",
    "bas_6_exp = gpd.read_file(output_path + '5rivers_all_controls_panel.shp')\n",
    "bas_6_exp.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist',\n",
    "                      'n_of_dam_i':'dams_in_c_per_y', 'dams_per_c':'dams_per_c_99'}, inplace=True)\n",
    "bas_6_exp['HYBAS_ID'] = bas_6_exp['HYBAS_ID'].apply(lambda x: str(x))\n",
    "bas_6_exp['PFAF_ID'] = bas_6_exp['PFAF_ID'].apply(lambda x: str(x))\n",
    "bas_6_exp['year'] = bas_6_exp['year'].apply(lambda x: int(x))\n",
    "# bas_6_exp = bas_6_exp[bas_6_exp['year'] < 2018]\n",
    "# bas_6_exp.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5eff63",
   "metadata": {},
   "source": [
    "# Conflicts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e75cab52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CRS of df is EPSG:3857\n",
    "df = gpd.read_file(conflict_path + '1997-01-01-2023-09-30.csv', sep=';', dtype={'timestamp': 'object'})\n",
    "gdf_conf = gpd.GeoDataFrame(df, geometry = gpd.points_from_xy(df.longitude, df.latitude))\n",
    "gdf_conf.set_crs('epsg:4326', inplace=True)\n",
    "del df\n",
    "# gdf_conf.set_crs('epsg:3857', inplace=True)\n",
    "# gdf_conf.to_crs(epsg=4326, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6750ce37",
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = ['Eastern Africa', 'Middle Africa', 'Northern Africa', 'Southern Africa', 'Western Africa']\n",
    "gdf_conf = gdf_conf[gdf_conf['region'].apply(lambda x: True if x in regions else False)]\n",
    "gdf_conf['year'] = gdf_conf['year'].apply(lambda x: int(x))\n",
    "# gdf_conf = gdf_conf[gdf_conf['year'] < 2018]\n",
    "gdf_conf.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "314a8896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s = gdf_conf[(gdf_conf['interaction'] == '18')][['actor1', 'actor2', 'notes']] – Beginning for cleaning of intrastate conflict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4d2d8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All conflicts:\n",
    "all_conflicts_gdf = gdf_conf.copy()\n",
    "\n",
    "# Riots:\n",
    "riots_gdf = gdf_conf[gdf_conf.event_type == 'Riots'].copy()\n",
    "riots_gdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Filter by battles:\n",
    "battles_gdf = gdf_conf[gdf_conf.event_type == 'Battles'].copy()\n",
    "battles_gdf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "del gdf_conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2cc1d045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99998882369376924646\r"
     ]
    }
   ],
   "source": [
    "# Match geography of battles with hydrobasins:\n",
    "joined = gpd.sjoin(bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']],\n",
    "                   all_conflicts_gdf[['event_id_cnty', 'event_date', 'geometry', 'year', 'fatalities', 'timestamp']], how='right')\n",
    "joined.drop(['index_left'], axis=1, inplace=True)\n",
    "joined[['day', 'month', 'year']] = joined['event_date'].str.split(' ', expand = True)\n",
    "joined['month_year'] = joined[['month', 'year']].apply(lambda x: ' '.join(x), axis=1)\n",
    "joined['month_year'] = joined['month_year'].apply(lambda x: datetime.strptime(x, '%B %Y'))\n",
    "joined['month'] = joined['month_year'].apply(lambda x: x.month)\n",
    "joined['year'] = joined['month_year'].apply(lambda x: x.year)\n",
    "\n",
    "# Create dict of lists of conflicts for every combination with of basin ID, month and year:\n",
    "basin_conf_dict = {}\n",
    "# for hydrobasin_id, confs_in_basin in joined.groupby(['HYBAS_ID', 'month', 'year']):\n",
    "for hydrobasin_id, confs_in_basin in joined.groupby(['HYBAS_ID', 'year']):\n",
    "    confs_ids = confs_in_basin['event_id_cnty'].tolist()\n",
    "    basin_conf_dict[hydrobasin_id] = confs_ids\n",
    "    \n",
    "# Match number of conflicts with basins, country and year:\n",
    "bas_6_exp['conflicts_per_y'] = 0\n",
    "for idx, row in bas_6_exp.iterrows():\n",
    "    print(idx/len(bas_6_exp), end='\\r')\n",
    "    # key = (row['HYBAS_ID'], row['month'], row['year'])\n",
    "    key = (row['HYBAS_ID'], row['year'])\n",
    "    if key in basin_conf_dict.keys():\n",
    "        bas_6_exp.loc[idx, 'conflicts_per_y'] = int(len(basin_conf_dict[key]))\n",
    "    else:\n",
    "        bas_6_exp.loc[idx, 'conflicts_per_y'] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15a22580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99998882369376924646\r"
     ]
    }
   ],
   "source": [
    "# Match geography of battles with hydrobasins:\n",
    "joined = gpd.sjoin(bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']],\n",
    "                   riots_gdf[['event_id_cnty', 'event_date', 'geometry', 'year', 'fatalities', 'timestamp']], how='right')\n",
    "joined.drop(['index_left'], axis=1, inplace=True)\n",
    "joined[['day', 'month', 'year']] = joined['event_date'].str.split(' ', expand = True)\n",
    "joined['month_year'] = joined[['month', 'year']].apply(lambda x: ' '.join(x), axis=1)\n",
    "joined['month_year'] = joined['month_year'].apply(lambda x: datetime.strptime(x, '%B %Y'))\n",
    "joined['month'] = joined['month_year'].apply(lambda x: x.month)\n",
    "joined['year'] = joined['month_year'].apply(lambda x: x.year)\n",
    "\n",
    "# Create dict of lists of conflicts for every combination with of basin ID, month and year:\n",
    "basin_conf_dict = {}\n",
    "# for hydrobasin_id, confs_in_basin in joined.groupby(['HYBAS_ID', 'month', 'year']):\n",
    "for hydrobasin_id, confs_in_basin in joined.groupby(['HYBAS_ID', 'year']):\n",
    "    confs_ids = confs_in_basin['event_id_cnty'].tolist()\n",
    "    basin_conf_dict[hydrobasin_id] = confs_ids\n",
    "    \n",
    "# Match number of conflicts with basins, country and year:\n",
    "bas_6_exp['riots_per_y'] = 0\n",
    "for idx, row in bas_6_exp.iterrows():\n",
    "    print(idx/len(bas_6_exp), end='\\r')\n",
    "    # key = (row['HYBAS_ID'], row['month'], row['year'])\n",
    "    key = (row['HYBAS_ID'], row['year'])\n",
    "    if key in basin_conf_dict.keys():\n",
    "        bas_6_exp.loc[idx, 'riots_per_y'] = int(len(basin_conf_dict[key]))\n",
    "    else:\n",
    "        bas_6_exp.loc[idx, 'riots_per_y'] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fc009a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99998882369376924646\r"
     ]
    }
   ],
   "source": [
    "# Match geography of battles with hydrobasins:\n",
    "joined = gpd.sjoin(bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']],\n",
    "                   battles_gdf[['event_id_cnty', 'event_date', 'geometry', 'year', 'fatalities', 'timestamp']], how='right')\n",
    "joined.drop(['index_left'], axis=1, inplace=True)\n",
    "joined[['day', 'month', 'year']] = joined['event_date'].str.split(' ', expand = True)\n",
    "joined['month_year'] = joined[['month', 'year']].apply(lambda x: ' '.join(x), axis=1)\n",
    "joined['month_year'] = joined['month_year'].apply(lambda x: datetime.strptime(x, '%B %Y'))\n",
    "joined['month'] = joined['month_year'].apply(lambda x: x.month)\n",
    "joined['year'] = joined['month_year'].apply(lambda x: x.year)\n",
    "\n",
    "# Create dict of lists of conflicts for every combination with of basin ID, month and year:\n",
    "basin_conf_dict = {}\n",
    "# for hydrobasin_id, confs_in_basin in joined.groupby(['HYBAS_ID', 'month', 'year']):\n",
    "for hydrobasin_id, confs_in_basin in joined.groupby(['HYBAS_ID', 'year']):\n",
    "    confs_ids = confs_in_basin['event_id_cnty'].tolist()\n",
    "    basin_conf_dict[hydrobasin_id] = confs_ids\n",
    "    \n",
    "# Match number of conflicts with basins, country and year:\n",
    "bas_6_exp['battles_per_y'] = 0\n",
    "for idx, row in bas_6_exp.iterrows():\n",
    "    print(idx/len(bas_6_exp), end='\\r')\n",
    "    # key = (row['HYBAS_ID'], row['month'], row['year'])\n",
    "    key = (row['HYBAS_ID'], row['year'])\n",
    "    if key in basin_conf_dict.keys():\n",
    "        bas_6_exp.loc[idx, 'battles_per_y'] = int(len(basin_conf_dict[key]))\n",
    "    else:\n",
    "        bas_6_exp.loc[idx, 'battles_per_y'] = int(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e3c0a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HYBAS_ID', 'NEXT_DOWN', 'NEXT_SINK', 'MAIN_BAS', 'DIST_SINK',\n",
       "       'DIST_MAIN', 'SUB_AREA', 'UP_AREA', 'PFAF_ID', 'ENDO', 'COAST', 'ORDER',\n",
       "       'SORT', 'RG', 'grad_15_3', 'grad_3_6', 'grad_more_6', 'tot_riv_dist',\n",
       "       'av_elev', 'shr_l_25', 'shr_25_50', 'shr_50_1k', 'shr_m_1k', 'has_dam',\n",
       "       'n_of_dams', 'Country', 'year', 'dams_per_c_99', 'dams_in_c_per_y',\n",
       "       'geometry', 'conflicts_per_y', 'riots_per_y', 'battles_per_y'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bas_6_exp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a05645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6_exp.battles_per_m_loop.value_counts()\n",
    "bas_6_exp['had_conf'] = bas_6_exp['conflicts_per_y'].apply(lambda x: 1 if x > 0 else 0)\n",
    "bas_6_exp['had_riot'] = bas_6_exp['riots_per_y'].apply(lambda x: 1 if x > 0 else 0)\n",
    "bas_6_exp['had_fight'] = bas_6_exp['battles_per_y'].apply(lambda x: 1 if x > 0 else 0)\n",
    "bas_6_exp['RGxD_hat'] = bas_6_exp['RG']*bas_6_exp['dams_in_c_per_y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "da1c5a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6_exp.to_file(output_path + '5rivers_prepared_data.shp')\n",
    "bas_6_exp = gpd.read_file(output_path + '5rivers_prepared_data.shp')\n",
    "bas_6_exp.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist',\n",
    "                      'dams_in_c_':'dams_in_c_per_y', 'dams_per_c':'dams_per_c_99',\n",
    "                      'conflicts_': 'confs_py', 'riots_per_':'riots_py', \n",
    "                      'battles_pe':'battles_py'}, inplace=True)\n",
    "# In case of weird column: dams_per_1 = dams_per_c_99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add dams as endogenous variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_to_use = 1999\n",
    "# dams_df = gpd.read_file(dams_path + 'GRanD_dams_v1_3.shp')\n",
    "dams_df = gpd.read_file(output_path + 'combined_dams.shp')\n",
    "\n",
    "# # Wherever main year is -99, so is alternative year:\n",
    "dams_df = dams_df[dams_df['YEAR'] != -99]\n",
    "dams_df.reset_index(inplace=True,drop=True)\n",
    "\n",
    "uses = {'Main', 'Major'}\n",
    "# uses = {'Main', 'Sec', 'Major'}\n",
    "\n",
    "dams_df['el_ir'] = dams_df.apply(lambda row: 1 if (row.USE_ELEC in uses) or \n",
    "                                 (row.USE_IRRI in uses) else 0, axis=1)\n",
    "dams_df = dams_df[dams_df['el_ir'] == 1]\n",
    "dams_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dams_1900_1999 = dams_df[(dams_df['YEAR'] >= 1900) & (dams_df['YEAR'] < year_to_use)].copy()\n",
    "dams_1900_1999.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dams_after_99 = dams_df[dams_df['YEAR'] >= year_to_use].copy()\n",
    "dams_after_99.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the amount of dams per basin by 1999:\n",
    "joined_1900_1999 = gpd.sjoin(dams_1900_1999[['GRAND_ID', 'YEAR', 'MAIN_USE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "joined_1900_1999 = joined_1900_1999[~joined_1900_1999['HYBAS_ID'].isna()]\n",
    "joined_1900_1999.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dam_bas_1900_99 = {}\n",
    "for bas_id, data in joined_1900_1999.groupby('HYBAS_ID'):\n",
    "    dam_ids = data['GRAND_ID'].to_list()\n",
    "    dam_bas_1900_99[bas_id] = dam_ids\n",
    "\n",
    "# Fill number of dams by 1999 for each basin with created dictionary values:\n",
    "bas_6_exp['basdams_99'] = bas_6_exp['HYBAS_ID'].apply(lambda x: len(dam_bas_1900_99[x]) if \n",
    "                                                      x in dam_bas_1900_99.keys() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the amount of dams per basin per year after 1999:\n",
    "joined_after_99 = gpd.sjoin(dams_after_99[['GRAND_ID', 'YEAR', 'MAIN_USE', 'geometry']],\n",
    "                   bas_6[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "joined_after_99 = joined_after_99[~joined_after_99['HYBAS_ID'].isna()]\n",
    "joined_after_99.reset_index(inplace=True, drop=True)\n",
    "\n",
    "dam_bas_after_99 = {}\n",
    "for bas_id, data in joined_after_99.groupby(['HYBAS_ID', 'YEAR']):\n",
    "    dam_ids = data['GRAND_ID'].to_list()\n",
    "    dam_bas_after_99[bas_id] = dam_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removed_dams = {1987: [1918],\n",
    "#                 2002: [2085],\n",
    "#                 2003: [769],\n",
    "#                 2005: [2006],\n",
    "#                 2007: [820, 2844],\n",
    "#                 2008: [772, 2882],\n",
    "#                 2013: [6315],\n",
    "#                 2016: [6285]}\n",
    "\n",
    "# dams_df[dams_df.REM_YEAR != -99].COUNTRY.value_counts()\n",
    "\n",
    "# dam_bas_after_99\n",
    "# dam_bas_1900_99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99998882369376924646\r"
     ]
    }
   ],
   "source": [
    "# Create the dict of total amount of dams in basin c for year t:\n",
    "basins = set(bas_6_exp.HYBAS_ID)\n",
    "tot_dams_per_basin_year = {}\n",
    "idx = 0\n",
    "for basin in basins:\n",
    "    idx += 1\n",
    "    print(idx/len(basins), end='\\r')\n",
    "    tot_dams_per_year = {}\n",
    "    for year in range(1999, 2024):\n",
    "        if year == 1999:\n",
    "            if basin in dam_bas_1900_99.keys():\n",
    "                tot_dams_per_year[year] = len(dam_bas_1900_99[basin])\n",
    "            else:\n",
    "                tot_dams_per_year[year] = 0\n",
    "        else:\n",
    "            key = (basin, year)\n",
    "            if key in dam_bas_after_99.keys():\n",
    "                tot_dams_per_year[year] = tot_dams_per_year[year-1] + len(dam_bas_after_99[key])\n",
    "            else:\n",
    "                tot_dams_per_year[year] = tot_dams_per_year[year-1]\n",
    "    tot_dams_per_basin_year[basin] = tot_dams_per_year\n",
    "    \n",
    "# Use the created dictionary to fill in the basins dataframe:\n",
    "bas_6_exp['dams_in_b_per_y'] = 0\n",
    "for idx, row in bas_6_exp.iterrows():\n",
    "    print(idx/len(bas_6_exp), end='\\r')\n",
    "    bas_6_exp.loc[idx, 'dams_in_b_per_y'] = tot_dams_per_basin_year[row['HYBAS_ID']][row['year']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_more_6\n",
      "tot_riv_dist\n",
      "dams_per_c_99\n",
      "dams_in_c_per_y\n",
      "dams_in_b_per_y\n"
     ]
    }
   ],
   "source": [
    "# bas_6_exp.drop(columns=['basdams_py'], inplace=True)\n",
    "\n",
    "col_names = list(bas_6_exp.columns)\n",
    "for col in col_names:\n",
    "    if len(col) > 10:\n",
    "        print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_rename = {'grad_more_6': 'grad_m_6',\n",
    "                  'tot_riv_dist': 'tot_riv_di',\n",
    "                  'dams_per_c_99': 'dams_pc_99',\n",
    "                  'dams_in_b_per_y': 'dams_pby',\n",
    "                  'n_of_dam_i': 'dams_pcy',\n",
    "                  'dams_in_c_per_y': 'dams_pcy',}\n",
    "\n",
    "bas_6_exp.rename(columns=cols_to_rename, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "17348789",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['HYBAS_ID', 'NEXT_DOWN', 'NEXT_SINK', 'MAIN_BAS', 'DIST_SINK',\n",
       "       'DIST_MAIN', 'SUB_AREA', 'UP_AREA', 'PFAF_ID', 'ENDO', 'COAST', 'ORDER',\n",
       "       'SORT', 'RG', 'grad_15_3', 'grad_3_6', 'grad_m_6', 'tot_riv_di',\n",
       "       'av_elev', 'shr_l_25', 'shr_25_50', 'shr_50_1k', 'shr_m_1k', 'has_dam',\n",
       "       'n_of_dams', 'Country', 'year', 'dams_pc_99', 'n_of_dam_i', 'confs_py',\n",
       "       'riots_py', 'battles_py', 'had_conf', 'had_riot', 'had_fight',\n",
       "       'RGxD_hat', 'geometry', 'basdams_99', 'dams_pby'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bas_6_exp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interaction between geographic controls and D_hat\n",
    "bas_6_exp['z_ieygg'] = bas_6_exp['grad_15_3']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_tkdgw'] = bas_6_exp['grad_3_6']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_sobhm'] = bas_6_exp['grad_m_6']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_fobki'] = bas_6_exp['tot_riv_di']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_hyeah'] = bas_6_exp['av_elev']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_oxjpe'] = bas_6_exp['SUB_AREA']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_vcjei'] = bas_6_exp['shr_l_25']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_nlvsk'] = bas_6_exp['shr_25_50']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_ahjvn'] = bas_6_exp['shr_50_1k']*bas_6_exp['dams_pcy']\n",
    "bas_6_exp['z_zgjij'] = bas_6_exp['shr_m_1k']*bas_6_exp['dams_pcy']\n",
    "\n",
    "columns_names_meanings = {\n",
    "    'z_ieygg': 'Share of river gradient between 1,5% and 3% in a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_tkdgw': 'Share of river gradient between 3% and 6% in a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_sobhm': 'Share of river gradient more than 6% in a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_fobki': 'Total distance of rivers in a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_hyeah': 'Average elevation of a basin multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_vcjei': 'Area of elevation lower than 250m as a share of basin area, multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_nlvsk': 'Area of elevation between 250m and 500m as a share of basin area, multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_ahjvn': 'Area of elevation between 500m and 1000m as a share of basin area, multiplied by total number of dams in a corresponding country in a corresponding year.',\n",
    "    'z_zgjij': 'Area of elevation between 500m and 1000m as a share of basin area, multiplied by total number of dams in a corresponding country in a corresponding year.'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bas_6_exp[\"centroid\"] = bas_6_exp[\"geometry\"].centroid\n",
    "bas_6_exp[\"lon\"] = bas_6_exp[\"centroid\"].apply(lambda p: p.x)\n",
    "bas_6_exp[\"lat\"] = bas_6_exp[\"centroid\"].apply(lambda p: p.y)\n",
    "bas_6_exp.drop(columns=['centroid'], inplace= True)\n",
    "bas_6_exp.to_file(output_path + '5rivers_fully_prepared_data.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b57586",
   "metadata": {},
   "source": [
    "# Advanced First Stage Check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "aa2aeb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bas_6_exp = gpd.read_file(output_path + '5rivers_fully_prepared_data.shp')\n",
    "bas_6_exp.rename(columns={'grad_more_':'grad_more_6', 'tot_riv_di':'tot_riv_dist',\n",
    "                      'dams_in_c_':'dams_in_c_per_y', 'dams_per_c':'dams_per_c_99',\n",
    "                      'conflicts_': 'confs_py', 'riots_per_':'riots_py', \n",
    "                      'battles_pe':'battles_py'}, inplace=True)\n",
    "\n",
    "iv = bas_6_exp[bas_6_exp['year'] == 1999].copy()\n",
    "iv.reset_index(drop=True,inplace=True)\n",
    "iv['has_dam'] = iv['basdams_99'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30688650",
   "metadata": {},
   "outputs": [],
   "source": [
    "dams_df = gpd.read_file(output_path + 'combined_dams.shp')\n",
    "\n",
    "any_dams_99 = dams_df[dams_df['YEAR'] <= 1999].copy()\n",
    "any_dams_99.reset_index(inplace=True,drop=True)\n",
    "\n",
    "# uses = {'Major'}\n",
    "uses = {'Main', 'Major'}\n",
    "# uses = {'Main', 'Sec', 'Major'}\n",
    "\n",
    "any_dams_99['el_ir'] = any_dams_99.apply(lambda row: 1 if (row.USE_ELEC in uses) or \n",
    "                                              (row.USE_IRRI in uses) else 0, axis=1)\n",
    "any_dams_99['el'] = any_dams_99.apply(lambda row: 1 if row.USE_ELEC in uses else 0, axis=1)\n",
    "any_dams_99['ir'] = any_dams_99.apply(lambda row: 1 if row.USE_IRRI in uses else 0, axis=1)\n",
    "\n",
    "dams_el_ir_99 = any_dams_99[any_dams_99['el_ir'] == 1]\n",
    "dams_el_ir_99.reset_index(inplace=True,drop=True)\n",
    "dams_el_99 = any_dams_99[any_dams_99['el'] == 1]\n",
    "dams_el_99.reset_index(inplace=True,drop=True)\n",
    "dams_ir_99 = any_dams_99[any_dams_99['ir'] == 1]\n",
    "dams_ir_99.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "41b15d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                has_dam   R-squared:                       0.034\n",
      "Model:                            OLS   Adj. R-squared:                  0.032\n",
      "Method:                 Least Squares   F-statistic:                     14.10\n",
      "Date:                Fri, 05 Jan 2024   Prob (F-statistic):           1.38e-22\n",
      "Time:                        16:42:50   Log-Likelihood:                 2010.6\n",
      "No. Observations:                3579   AIC:                            -4001.\n",
      "Df Residuals:                    3569   BIC:                            -3939.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        0.0051      0.007      0.776      0.438      -0.008       0.018\n",
      "SUB_AREA     -8.083e-07   6.78e-07     -1.192      0.233   -2.14e-06    5.21e-07\n",
      "av_elev      -1.565e-05   2.28e-05     -0.685      0.493   -6.04e-05    2.91e-05\n",
      "shr_25_50       -0.0094      0.010     -0.926      0.354      -0.029       0.010\n",
      "shr_50_1k       -0.0101      0.016     -0.632      0.528      -0.041       0.021\n",
      "shr_m_1k         0.0061      0.029      0.211      0.833      -0.050       0.062\n",
      "tot_riv_dist  1.252e-08   3.11e-09      4.023      0.000    6.42e-09    1.86e-08\n",
      "grad_15_3        0.2274      0.065      3.496      0.000       0.100       0.355\n",
      "grad_3_6        -0.1821      0.148     -1.231      0.218      -0.472       0.108\n",
      "grad_m_6         0.3974      0.143      2.774      0.006       0.116       0.678\n",
      "==============================================================================\n",
      "Omnibus:                     4223.064   Durbin-Watson:                   1.986\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           286109.941\n",
      "Skew:                           6.487   Prob(JB):                         0.00\n",
      "Kurtosis:                      44.836   Cond. No.                     2.33e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.33e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                 Coef:     P>|t|\n",
      "0  grad_15_3  0.227396  0.000478\n",
      "1   grad_3_6 -0.182052  0.218278\n",
      "2   grad_m_6  0.397399  0.005571\n"
     ]
    }
   ],
   "source": [
    "# Hydropower dams before 1999:\n",
    "joined = gpd.sjoin(dams_el_99[['GRAND_ID', 'YEAR', 'MAIN_USE', 'geometry']],\n",
    "                   iv[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids    \n",
    "iv['has_dam'] = iv['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "df1 = pd.DataFrame(iv.drop(columns='geometry'))\n",
    "\n",
    "first_stage1 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_m_6\",\n",
    "                      data=df1).fit()\n",
    "\n",
    "print(first_stage1.summary())\n",
    "rows, model = [], first_stage1\n",
    "for idx in range (len(first_stage1.params)-3, len(first_stage1.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "73cc20bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                has_dam   R-squared:                       0.061\n",
      "Model:                            OLS   Adj. R-squared:                  0.059\n",
      "Method:                 Least Squares   F-statistic:                     25.87\n",
      "Date:                Fri, 05 Jan 2024   Prob (F-statistic):           1.29e-43\n",
      "Time:                        17:18:39   Log-Likelihood:                -691.82\n",
      "No. Observations:                3579   AIC:                             1404.\n",
      "Df Residuals:                    3569   BIC:                             1465.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        0.0407      0.014      2.942      0.003       0.014       0.068\n",
      "SUB_AREA       8.68e-07   1.44e-06      0.602      0.547   -1.96e-06     3.7e-06\n",
      "av_elev       1.822e-06   4.86e-05      0.037      0.970   -9.35e-05    9.71e-05\n",
      "shr_25_50       -0.0164      0.021     -0.763      0.446      -0.059       0.026\n",
      "shr_50_1k       -0.0555      0.034     -1.637      0.102      -0.122       0.011\n",
      "shr_m_1k         0.0448      0.061      0.732      0.464      -0.075       0.165\n",
      "tot_riv_dist  1.344e-08   6.62e-09      2.029      0.043    4.53e-10    2.64e-08\n",
      "grad_15_3        0.4921      0.138      3.556      0.000       0.221       0.763\n",
      "grad_3_6         0.8029      0.315      2.552      0.011       0.186       1.420\n",
      "grad_m_6        -0.5730      0.305     -1.880      0.060      -1.171       0.025\n",
      "==============================================================================\n",
      "Omnibus:                     1649.459   Durbin-Watson:                   1.442\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             6291.818\n",
      "Skew:                           2.385   Prob(JB):                         0.00\n",
      "Kurtosis:                       7.409   Cond. No.                     2.33e+08\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.33e+08. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                 Coef:     P>|t|\n",
      "0  grad_15_3  0.492096  0.000382\n",
      "1   grad_3_6  0.802904  0.010745\n",
      "2   grad_m_6 -0.572975  0.060257\n"
     ]
    }
   ],
   "source": [
    "# All dams before 1999:\n",
    "joined = gpd.sjoin(any_dams_99[['GRAND_ID', 'YEAR', 'MAIN_USE', 'geometry']],\n",
    "                   iv[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids    \n",
    "iv['has_dam'] = iv['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "df1 = pd.DataFrame(iv.drop(columns='geometry'))\n",
    "\n",
    "first_stage1 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + grad_15_3 + grad_3_6 + grad_m_6\",\n",
    "                      data=df1).fit()\n",
    "\n",
    "print(first_stage1.summary())\n",
    "rows, model = [], first_stage1\n",
    "for idx in range (len(first_stage1.params)-3, len(first_stage1.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e1a5fd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                has_dam   R-squared:                       0.034\n",
      "Model:                            OLS   Adj. R-squared:                  0.032\n",
      "Method:                 Least Squares   F-statistic:                     17.81\n",
      "Date:                Fri, 05 Jan 2024   Prob (F-statistic):           2.18e-23\n",
      "Time:                        17:21:47   Log-Likelihood:                 2009.5\n",
      "No. Observations:                3579   AIC:                            -4003.\n",
      "Df Residuals:                    3571   BIC:                            -3954.\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "Intercept        0.0050      0.006      0.791      0.429      -0.007       0.018\n",
      "SUB_AREA     -8.268e-07   6.78e-07     -1.220      0.223   -2.16e-06    5.02e-07\n",
      "av_elev      -1.319e-05   2.16e-05     -0.611      0.541   -5.55e-05    2.91e-05\n",
      "shr_25_50       -0.0102      0.010     -1.021      0.307      -0.030       0.009\n",
      "shr_50_1k       -0.0118      0.015     -0.785      0.433      -0.041       0.018\n",
      "shr_m_1k         0.0033      0.027      0.121      0.903      -0.050       0.057\n",
      "tot_riv_dist  1.258e-08   3.11e-09      4.042      0.000    6.48e-09    1.87e-08\n",
      "RG               0.1897      0.031      6.113      0.000       0.129       0.250\n",
      "==============================================================================\n",
      "Omnibus:                     4225.913   Durbin-Watson:                   1.987\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):           286830.617\n",
      "Skew:                           6.494   Prob(JB):                         0.00\n",
      "Kurtosis:                      44.890   Cond. No.                     4.35e+07\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.35e+07. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "                        Coef:         P>|t|\n",
      "0      shr_m_1k  3.319708e-03  9.034780e-01\n",
      "1  tot_riv_dist  1.258295e-08  5.401991e-05\n",
      "2            RG  1.896683e-01  1.081738e-09\n"
     ]
    }
   ],
   "source": [
    "# All dams before 1999:\n",
    "joined = gpd.sjoin(dams_el_99[['GRAND_ID', 'YEAR', 'MAIN_USE', 'geometry']],\n",
    "                   iv[['HYBAS_ID', 'PFAF_ID', 'geometry', 'RG']], op='within', how='left')\n",
    "basin_dam_dict = {}\n",
    "for hydrobasin_id, dams_in_basin in joined.groupby('HYBAS_ID'):\n",
    "    dam_ids = dams_in_basin['GRAND_ID'].tolist()\n",
    "    basin_dam_dict[hydrobasin_id] = dam_ids\n",
    "iv['has_dam'] = iv['HYBAS_ID'].apply(lambda x: 1 if x in basin_dam_dict.keys() else 0)\n",
    "df1 = pd.DataFrame(iv.drop(columns='geometry'))\n",
    "\n",
    "first_stage1 = smf.ols(\"has_dam ~ SUB_AREA + av_elev + shr_25_50 + shr_50_1k + shr_m_1k + tot_riv_dist + RG\",\n",
    "                      data=df1).fit()\n",
    "\n",
    "print(first_stage1.summary())\n",
    "rows, model = [], first_stage1\n",
    "for idx in range (len(first_stage1.params)-3, len(first_stage1.params)):\n",
    "    rows.append([model.params.index[idx], model.params[idx], model.pvalues[idx]])\n",
    "print(pd.DataFrame(rows, columns=[' ', 'Coef:', 'P>|t|']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
